<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lbx&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sec-lbx.tk/"/>
  <updated>2017-07-28T09:04:47.000Z</updated>
  <id>http://sec-lbx.tk/</id>
  
  <author>
    <name>Benxi Liu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅谈GCC编译优化</title>
    <link href="http://sec-lbx.tk/2017/07/15/%E6%B5%85%E8%B0%88GCC%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/"/>
    <id>http://sec-lbx.tk/2017/07/15/浅谈GCC编译优化/</id>
    <published>2017-07-15T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="GCC-Pass"><a href="#GCC-Pass" class="headerlink" title="GCC Pass"></a>GCC Pass</h4><p>在GCC完成词法、语法分析，并获得源代码对应的抽象语法树AST之后，会将其转换为对应的GIMPLE序列。随后，GCC会对GIMPLE中间表示进行一系列的处理，包括GIMPLE的低级化、优化、RTL生成等、RTL优化等。为了方便管理，GCC采用了一种称为Pass的组织形式，把它们分成一个个的处理过程，把每个输出结果作为下一个处理过程的输入。<br>GCC中，Pass可以分为4类，GIMPLE_PASS，RTL_PASS，SIMPLE_IPA_PASS，IPA_PASS。其中除了RTL_PASS之外，处理对象都是GIMPLE中间表示。名字包含IPA的两类Pass的功能主要是过程分析，也即函数间调用和传递。  Pass的执行是以链表的形式组织的，每个Pass还可以包含子Pass，并且以函数为执行的基本单元。<br>如果把GCC中的Pass全部打印出来，可以看到数量有数百个，因此这里只选择几个代表性的pass研究一下。值得说明的是，在GCC 4.6之后，增加了插件功能，它同样是通过Pass的形式来进行管理，这使得我们自定义处理过程变得很容易。    </p>
<h4 id="去除无用表达式"><a href="#去除无用表达式" class="headerlink" title="去除无用表达式"></a>去除无用表达式</h4><p>该Pass从GIMPLE序列中进行搜索，从中删除死代码。这些死代码主要包括：<br>（1）空语句<br>（2）无意义的语句块、条件表达式<br>（3）目标地址就是下一条段GOTO表达式<br>（4）无意义的malloc、free<br>（5）…<br>在GCC 4.4版本中，这个Pass是<code>pass_remove_useless_stmts</code>，现在这个函数已经被移除了；其功能被拆分到了别的函数中。但gcc的文档中还是说明了这个Pass。<br>类似这样的结构，在这个Pass中都会被优化掉：</p>
<pre><code>if(1)        //无意义的条件表达式

goto next;    //无意义的goto
next:
    //do sth
</code></pre><p>那么该Pass是如何进行判断的呢？我们来看看GCC 5.4版本中，类似函数的实现。在gcc/tree-ssa-dce.c当中，有这样一个函数<code>eliminate_unnecessary_stmts()</code>。这个函数会(逆序的)逐个遍历表达式，防止某些定义或名字被释放：</p>
<pre><code>for(gsi = gsi_last_bb (bb); !gsi_end_p (gsi); gsi = psi){
    stmt = gsi_stmt (gsi);
    ...
    if (gimple_call_with_bounds_p (stmt)) {
        //如果有必要删除，那么会首先利用这个函数设置
        gimple_set_plt(...)
    }
    //对于需要删除的表达式进行删除
    if (!gimple_plf (stmt, STMT_NECESSARY))
    ...
}
</code></pre><p>如果进行了删除，那么这个函数还会对一部分CFG进行修改，并删除不可达的基本块。</p>
<h4 id="控制流图的构造"><a href="#控制流图的构造" class="headerlink" title="控制流图的构造"></a>控制流图的构造</h4><p>在GCC中，控制流图指的是<strong>函数内的控制流图</strong>，这和我平时在文章中看到的“CFG”是不同的。GCC中，CFG的节点为基本块，而边则是基本块之间的跳转关系。<code>pass_build_cfg</code>对函数对GIMPLE序列进行分析，完成基本块的划分，并且根据GIMPLE语义来构造基本块之间的跳转关系。<br>控制流图的构造，主要是在<code>build_gimple_cfg()</code>当中完成的。</p>
<pre><code>static void
build_gimgple_cfg (gimple_seq seq)
{
    gimple_register_cfg_hooks();//注册这个函数，要构造cfg了
    init_empty_tree_cfg();//先构造一个空的cfg
    make_blocks(seq);//具体的基本块构造过程
    ...
    make_edges();//创建基本块的边
}

static void
make_blocks (gimple_seq seq)
{
    while (!gsi_end_p (i))//遍历seq中的每一条
    {
        ...    
        gimple_set_bb (stmt, bb);//把当前的语句添加到基本块
        if(stmt_ends_bb_p(stmt))
        {
        //如果stmt是基本块的终结，就进行处理，并且在下一次创建一个新的基本块
        ...
        }
    }
}
</code></pre><p>在基本块创建之后，就可以根据基本块，构造基本块中的边，对于函数中的不同情况，生成了不同的边，并且创建了对应的label。</p>
<pre><code>static void
make_edges (basic_block min, basic_block max, int updata_p)
{
    ...
    //处理min和max之间的基本块
    FOR_BB_BETWEEN (bb, min, max-&gt;next_bb, next_bb)
    {
        //在5.4版本中，是直接对RTL进行处理
        rtx_insn *insn;
        insn = BB_END(bb);
        //如果为JUMP指令，还包括了条件跳转
        if(code == JUMP_INSN)
        ...
        //如果为CALL指令，还包括了异常处理
        else if(code == CALL_INSN || cfun-&gt;can_throw_non_call_exceptions)
        ...
    }
}
</code></pre><h4 id="指令调度"><a href="#指令调度" class="headerlink" title="指令调度"></a>指令调度</h4><p>前面介绍了GCC当中，GIMPLE表达式的部分优化过程，现在我们来说说RTL中的优化过程。指令调度是对当前函数中的insn序列进行重新排列，从而更充分地利用目标机器的硬件资源，提高指令执行的效率。指令调度主要考虑有数据的依赖关系、控制相关性、结构相关、指令延迟和指令代价等，通常指令调度与目标架构上的流水线有关。在GCC中，指令调度分为两个部分：寄存器分配之前的pass_sched和寄存器分配之后的pass_sched2。<br>不过，在这里应该先阐述一下：为什么要进行指令调度？这是由硬件的特性决定的：<br>（1）指令拥有不同的执行时间。对于指令来说，其实latency（指令开始执行后，多少时钟数能够为其他指令提供数据）、throughtput（指令完成计算需要的时钟数）、μops（指令包含的微操作数）都是不同的。<br>（2）指令流水线能够并行执行指令的微操作，从而在等待时完成一部分工作。<br>（3）超标量处理器，比如现在的i7处理器，能够同时运行多条指令，虽然硬件完成了一部分调度工作，但存在一个窗口的问题，依然需要编译器进行调度。<br>正因如此，编译器需要对指令进行调度，来提高运行时的效率。指令在调度时，存在着以下限制：<br>（1）数据依赖关系。包括flow（一条指令定义的值和寄存器在后面的指令用到）、anti（一条指令用的值或者寄存器会被后面的指令修改）、output（一条指令定义的值或寄存器在后面会被改；<br><!--（2）控制依赖关系。也即涉及到控制流转移的指令，例如if语句等； --><br>（2）循环限制，多次迭代之间的关系，循环内部的依赖关系等；<br>（3）硬件资源的限制，例如能够同时并行的指令数；<br>这些限制必须作为指令调度的输入被考虑到，通过这些依赖关系，可以把所有的指令，构成一个依赖关系图。<br>指令调度是一个典型的NP-hard问题，而<strong>表调度算法</strong>就是一种典型的启发式算法。通常其调度的单元在<strong>基本块的内部</strong>，并把数据依赖关系图作为输入，并计算出指令的优先级。优先级的计算涉及：依赖图的根到节点的路径长度、某个节点的后继数量、节点的latency等。<br>随后，保持两个表：<strong>ready</strong>表中保存了能够不延时执行的指令，它们根据优先级从高到低排列；<strong>active</strong>表包括了所有正在执行的指令。在算法的每一步：<br>（1）从<strong>ready</strong>表中取出优先级最高的指令进行调度，将它移到<strong>active</strong>队列当中，它会在队列中呆上一个时延的时间；<br>（2）更新指令的依赖关系，把新的就绪指令插入<strong>ready</strong>表中。这里我们用一个例子来说明：<br>在每一轮，都取了<strong>ready</strong>队列中，优先级最高的指令，如果<strong>active</strong>中的指令没有执行完，导致依赖关系无法满足，那么<strong>ready</strong>可能是空的，此时就继续下一轮调度。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%BC%96%E8%AF%91%E5%99%A8/%E8%A1%A8%E8%B0%83%E5%BA%A6.png?raw=true" alt="">  </p>
<table>
<thead>
<tr>
<th>cycle</th>
<th>ready</th>
<th>active  </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>a13, c12, e10, g8</td>
<td>a</td>
</tr>
<tr>
<td>2</td>
<td>c12, e10, g8</td>
<td>a, c</td>
</tr>
<tr>
<td>3</td>
<td>e10, g8</td>
<td>a, c, e</td>
</tr>
<tr>
<td>4</td>
<td>b10, g8</td>
<td>b, c, e</td>
</tr>
<tr>
<td>5</td>
<td>d9, g8</td>
<td>d, e</td>
</tr>
<tr>
<td>6</td>
<td>g8</td>
<td>d, g</td>
</tr>
<tr>
<td>7</td>
<td>f7</td>
<td>f, g</td>
</tr>
<tr>
<td>8</td>
<td></td>
<td>f, g</td>
</tr>
<tr>
<td>9</td>
<td>h5</td>
<td>h</td>
</tr>
<tr>
<td>10</td>
<td></td>
<td>h</td>
</tr>
<tr>
<td>11</td>
<td>i3</td>
<td>i  </td>
</tr>
</tbody>
</table>
<p>GCC中默认的指令调度算法是<strong>表调度算法</strong>。其处理过程在/gcc/sched-rgn.c中定义。<code>schedule_insns()</code>是调度的主要函数，它会执行两次，分别在：<br>（1）寄存器分配之前，在<code>pass_sched</code>中调用，实现以区域为调度范围的指令调度；<br>（2）寄存器分配之后，在<code>pass_sched2</code>中调用，通常只在每个基本块内部进行指令调度；  </p>
<pre><code>schedule_insns()
{
    int rgn;
    //如果没有包含代码的基本块，直接退出
    if (n_basic_blocks_for_fn (cfun) == NUM_FIXED_BLOCKS)
        return;
    //初始化指令调度信息
    rgn_setup_common_sched_info ();
    //初始表调度的基本信息
      rgn_setup_sched_infos ();
    //haifa表调度的数据结构初始化，进行数据流分析
      haifa_sched_init ();
      //初始化区域信息，将CFG的区域提取出来
      sched_rgn_init (reload_completed);

      bitmap_initialize (&amp;not_in_df, 0);
      bitmap_clear (&amp;not_in_df);    

      //对每个区域内的基本块进行调度
      for (rgn = 0; rgn &lt; nr_regions; rgn++)
    if (dbg_cnt (sched_region))
          schedule_region (rgn);

      //清除
      sched_rgn_finish ();
      bitmap_clear (&amp;not_in_df);
      haifa_sched_finish ();
}  
</code></pre><h4 id="统一寄存器分配"><a href="#统一寄存器分配" class="headerlink" title="统一寄存器分配"></a>统一寄存器分配</h4><p>我们知道，RTL在生成和处理的过程中，大量地使用了虚拟寄存器，那么这些虚拟寄存器在转换为目标机器的汇编码之前，需要映射到目标机器中的物理寄存器上，该过程即为寄存器分配。那么合理地分配、使用物理寄存器，是GCC后端及其重要的一个任务。<br>GCC中采用的是一种把寄存器合并、寄存器生存范围划分、寄存器优选、代码生成和染色整合在一起的算法，所以被称为统一寄存器分配。整个统一寄存器分配是这样一个过程：  </p>
<p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%BC%96%E8%AF%91%E5%99%A8/ira.png?raw=true" alt="">  </p>
<p>第一步是建立IRA的中间表示，其代码在ira-build.c中，入口函数为<code>ira-build()</code>；<br><strong>第二步</strong>是寄存器的着色。这一步按照自顶向下的顺序，遍历每个区域，对于区域中的分配元进行着色，由<code>ira_color()</code>函数完成；<br>第三步是代码移动，解决父子区域中，寄存器被spill/store的问题，如果子区域需要spill到内存，那么父区域也可以直接spill到内存，这样就不用多次进行移入内存/存内存取值到寄存器的过程；<br>第四步是代码修改，在着色处理后，父子区域中，相同虚拟寄存器可能会分配到不同的物理寄存器活着存储位置，而某个区域内部也可能会会对同一个虚拟寄存器分配不同的物理寄存器。此时IRA就分配新的虚拟寄存器进行取代，并且在区域边界实现新的分配元并进行交换；<br>第五步：将所有区域的分配元合并到一个区域中；<br>第六步：尝试对spill操作的分配元分配物理寄存器；<br>第七步：Reload Pass。前面的过程可能会引入新的代码、虚拟寄存器，产生不能满足模版约束的情况，reload处理这些情况，因此要重新进行第二步开始的操作，直到不再产生新的代码。  </p>
<p>这里只说明一下寄存器着色的过程。在第一步中，我们能够根据每个虚拟寄存器的生存范围，能够确定它们的冲突关系：生存范围有冲突的虚拟寄存器，不能被分配到同一个硬件寄存器当中去（<code>ira_build_conflicts()</code>的工作）。而寄存器着色，就是根据虚拟寄存器之间的冲突关系图，进行图的着色处理。在这个图中，每个节点代表需要着色的虚拟寄存器，而每条边都定义了冲突关系，也就是这两个虚拟寄存器不能着相同的颜色。那么如果有N个寄存器可以分配，就相当于用N个颜色来着色；如果N不够大，那么就需要物理内存来保存虚拟寄存器的值。  </p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>编译器的优化是一个及其复杂的过程，这里我只是从几个点出发，了解它的工作过程，对编译优化形成一个浅薄的认识。日后如果真的要从事编译优化方面的研究，再好好看看编译器的代码吧！ </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;GCC-Pass&quot;&gt;&lt;a href=&quot;#GCC-Pass&quot; class=&quot;headerlink&quot; title=&quot;GCC Pass&quot;&gt;&lt;/a&gt;GCC Pass&lt;/h4&gt;&lt;p&gt;在GCC完成词法、语法分析，并获得源代码对应的抽象语法树AST之后，会将其转换为对应的GIM
    
    </summary>
    
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
      <category term="编译器" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>从虚函数的实现，到虚表劫持攻击</title>
    <link href="http://sec-lbx.tk/2017/06/27/%E8%99%9A%E5%87%BD%E6%95%B0%EF%BC%8C%E5%8E%9F%E7%90%86%E5%92%8C%E6%94%BB%E5%87%BB%E6%96%B9%E5%BC%8F/"/>
    <id>http://sec-lbx.tk/2017/06/27/虚函数，原理和攻击方式/</id>
    <published>2017-06-27T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="虚函数与多态"><a href="#虚函数与多态" class="headerlink" title="虚函数与多态"></a>虚函数与多态</h4><p>继承和多态，是面向对象中老生常谈的话题。C++中，我们也可以经常看到<code>virtaul</code>、<code>override</code>这样的关键字；这正是虚函数的标志。虚函数就是为了解决多态的问题：如果要使用一个基类的指针，根据对象的不同类型去调用相应的函数，就需要使用虚函数了。通俗的说也就是同一个入口，却能够调用不同的方法。<br>通常，对于虚函数的调用，往往在运行时才能确定调用哪个版本的函数。这是由于基类的指针或者引用，其动态类型必须在运行的时候才能确定（它具体指向了什么类型）。而“动态绑定”就指的在运行时，根据对象的类型，调用具体方法的过程，这个过程正是通过<strong>虚函数表</strong>实现的。<br><strong>抽象基类</strong>指的是有<strong>纯虚函数</strong>的类。纯虚函数，指的是没有函数体的函数，通常通过在函数体的位置写上<code>=0</code>来表示。对于抽象基类，是不能直接创建一个对象的；但是可以创建它它们的派生类的对象：只要它们覆盖了纯虚函数。纯虚函数表示这个函数的具体实现全部交给派生类去做。  </p>
<h4 id="虚函数表与虚函数的调用"><a href="#虚函数表与虚函数的调用" class="headerlink" title="虚函数表与虚函数的调用"></a>虚函数表与虚函数的调用</h4><p>那么，“动态绑定”是如何实现的呢？这便是借助于虚函数表来实现。对于每个具有虚函数的类，都会有一个对应的虚函数表vtable，其代码和对应内存结构如下所示：</p>
<pre><code>class A {
    int varA;
    public:
    virtual int vAfoo(int a, int *b){
        return a + (*b);
    }
    virtual int vAbar(int a){
        return a + 1;
    }
    virtual bool vAduh(){
        return true;
    }
    virtual int vAtest(int a){
        return 0;
    }
    void Afoo(){
        this-&gt;vAduh();
    }
};

class B {
    int varB;
    public:
    virtual int vBfoo(int a) = 0;
    virtual bool vBbar(int b){
        return b == 0;
    }
    char *Bfoo(char *c){
        return c;
    }
};

class C : public A, public B {
    int varC;
    public:
    int vAtest(int a){
        return -(a);
    }
    int vAfoo(int a, int *b){
        return *b;
    }
    int vBfoo(int a){
        return a - 1;
    }
    virtual void vCfoo(){}
    bool vAduh(){
        return false;
    }
};
</code></pre><p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/c++/%E8%99%9A%E8%A1%A8.png?raw=true" alt="虚函数表"><br>这个表中的每一项，都是一个虚函数的地址，也就是虚函数的指针。而每个对象的第一个值都是虚标指针，它指向了了所对应虚函数表的第一个表项（也就是虚函数表的基址）。每次调用虚函数时，都会首先通过这个虚表指针，找到虚函数表，然后再在虚函数表中，找到真正的虚函数的地址，并进行调用。假设存在有多继承的情况，那么就会有多个vptr，分别放在对应的基类对象的开头位置。  </p>
<h4 id="虚继承"><a href="#虚继承" class="headerlink" title="虚继承"></a>虚继承</h4><p>对于“菱形继承”情况（也即两个子类继承同一个父类，而新的子类又同时继承这两个子类），则可能产生二义性问题。例如下面的情况，那么D中就会保存两次A中的变量和函数，并且在使用时也会很不方便，必须利用域作用符来使用变量和函数。  </p>
<pre><code>   A
 /   \
B1   B2
 \   /
   D
</code></pre><p>虚继承是在继承时，在基类类型前面加上<code>virtual</code>关键字。虚继承能够解决基类多副本的问题：在任何派生类当中，虚基类都是通过一个共享对象来表示的，它们通过指针去访问这个基类中的内容；它不用去保存多份基类的拷贝，而是只需要多出一个指向基类子对象的指针。从内存布局上来说，在虚表的负offset位置，会保存一个指针指向虚基类对象。<br>也就是说继承自A的虚函数和对象，全部只保存一份在D自身的子对象中，相比不使用虚继承，它删除了B1和B2当中的（2份）基类成员；它自己则需要保存一份基类成员和偏移指针；而如果要用B1和B2的指针或者引用去访问一个D对象时，那么访问A的成员则需要通过间接引用来访问；也就是说子对象需要有一个<strong>偏移量</strong>，指示在内存中，基类的位置。其内存布局一般如下：</p>
<table>
<thead>
<tr>
<th>内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>B1的虚表指针</td>
<td></td>
</tr>
<tr>
<td><strong>B1的偏移指针</strong></td>
<td></td>
</tr>
<tr>
<td>B1的数据成员</td>
<td></td>
</tr>
<tr>
<td>B2的虚表指针</td>
<td></td>
</tr>
<tr>
<td><strong>B2的偏移指针</strong></td>
<td></td>
</tr>
<tr>
<td>B2的数据成员</td>
<td></td>
</tr>
<tr>
<td>D的虚表指针</td>
<td></td>
</tr>
<tr>
<td><strong>D的偏移指针</strong></td>
<td></td>
</tr>
<tr>
<td>D的数据成员</td>
<td></td>
</tr>
<tr>
<td>A的虚表指针</td>
<td></td>
</tr>
<tr>
<td>A的数据成员</td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="虚表与劫持攻击"><a href="#虚表与劫持攻击" class="headerlink" title="虚表与劫持攻击"></a>虚表与劫持攻击</h4><p>在C++程序中，%90以上的间接调用都是vcall。篡改程序中的虚函数调用，是劫持C++程序的一种常见手段。这里简单说说常见手段。<br>一种方法是<strong>虚表注入</strong>。众所周知，虚表保存在程序的.rodata段中，它是可读，不可写的；而对象当中的虚表指针却是可读写的状态；因此篡改虚表指针是较为直接的方式。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/c++/%E8%99%9A%E8%A1%A8%E6%B3%A8%E5%85%A5.png?raw=true" alt="虚表注入"><br>如图，如果利用漏洞（overflow、use-after-free等）在内存中构造一个虚假的虚表，并且将对象中的虚函数指针指向注入的虚假的虚表，那么在虚函数调用时，就会调用虚假的虚函数。甚至只需要一次虚函数调用就能够通过shellcode完成攻击。<br>当然，如果程序进行了一定程度的保护，例如检查虚表指针是否属于.rodata段，攻击就只能依赖于现有的虚表来构造了。<a href="http://syssec.rub.de/media/emma/veroeffentlichungen/2015/03/28/COOP-Oakland15.pdf" target="_blank" rel="external">Counterfeit Object-oriented Programming</a>就提出了这样一种方法。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/c++/coop%E6%94%BB%E5%87%BB.png?raw=true" alt="COOP"><br>可以看到，这种方法没有注入新的虚表，而是将vptr的值，指向了虚表中的不同位置（而不是虚表的起始地址）。如果能够构造一系列的虚假对象，那么就可以在一次循环中（比如某个对象数组的依次析构），在调用同一个虚函数时，实际上调用不同的函数，从而构造一个虚假的执行链。看到这里，也许你会有疑问：仅仅用有限的虚函数，能够构造图灵计算的攻击吗？答案是肯定的：有兴趣的话可以阅读一下原文，通过拼凑虚函数，是能够组合出各种语义的。  </p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>可见，虚函数是面向对象语言中，十分巧妙而又必不可少的设计；但它的特点也使得它成为黑客滥用、攻击的目标。指的庆幸的是，目前已经有一些开销较小的方法，能够保护虚表和虚函数了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;虚函数与多态&quot;&gt;&lt;a href=&quot;#虚函数与多态&quot; class=&quot;headerlink&quot; title=&quot;虚函数与多态&quot;&gt;&lt;/a&gt;虚函数与多态&lt;/h4&gt;&lt;p&gt;继承和多态，是面向对象中老生常谈的话题。C++中，我们也可以经常看到&lt;code&gt;virtaul&lt;/code&gt;
    
    </summary>
    
    
      <category term="内存安全" scheme="http://sec-lbx.tk/tags/%E5%86%85%E5%AD%98%E5%AE%89%E5%85%A8/"/>
    
      <category term="虚函数" scheme="http://sec-lbx.tk/tags/%E8%99%9A%E5%87%BD%E6%95%B0/"/>
    
      <category term="C++" scheme="http://sec-lbx.tk/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>编译链中的一环，静态链接详解</title>
    <link href="http://sec-lbx.tk/2017/06/19/%E7%BC%96%E8%AF%91%E9%93%BE%E4%B8%AD%E7%9A%84%E4%B8%80%E7%8E%AF%EF%BC%8C%E9%9D%99%E6%80%81%E9%93%BE%E6%8E%A5%E8%AF%A6%E8%A7%A3/"/>
    <id>http://sec-lbx.tk/2017/06/19/编译链中的一环，静态链接详解/</id>
    <published>2017-06-19T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>GCC的工作，到生成汇编代码为止。剩下的工作，交给了Binutils来完成：assembler和static linker。最近详细地研究了一下linker的工作过程。<br>linker主要完成的是静态链接，目标文件合并的工作。例如，把多个.o文件合并成一个可执行文件。  </p>
<h4 id="两步链接"><a href="#两步链接" class="headerlink" title="两步链接"></a>两步链接</h4><p>两步链接指的是：  </p>
<ol>
<li>空间与地址的分配<br>链接器会首先扫描所有的输入文件，获得各个段的长度、属性和位置，将段合并；并将输入目标文件中的符号表合并为全局符号表。  </li>
<li>符号解析与重定位<br>使用上一步中收集到的信息，进行符号解析和重定位，调整代码中的地址等。这一步也是链接过程的核心，特别是重定位过程。<br>链接器首先获取各个段的虚拟地址；在确定段的虚拟地址之后，也就能确定各个符号的虚拟地址了。  </li>
</ol>
<h4 id="重定位与符号解析"><a href="#重定位与符号解析" class="headerlink" title="重定位与符号解析"></a>重定位与符号解析</h4><p>在完成空间和地址的分配之后，链接器开始进行符号解析和重定位的过程。在链接之前，各个段中的符号地址，都是以0为基地址的，对于未知的地址，也通通用0进行替代。编译器在编译时，对于不知道的符号地址，全部用一个假值替代，把真正的工作留给链接器去做。<br>而链接器在分配了虚拟地址之后，就可以修正每一个需要重定位的入口。这个工作是借助于重定位表来实现的。重定位表包括：<strong>重定位入口</strong>（也就是需要重定位的地方），<strong>偏移</strong>表示入口在被重定位的段中的位置。<br>在x86_64下，重定位表的结构也很简单（定义在elf.h当中）：  </p>
<pre><code>typedef struct{
    Elf64_Addr r_offset;
    Elf64_Xword r_info;
}Elf64_Rel;

typedef struct{
    Elf64_Addr r_offset;
    Elf64_Xword r_info;
    ELF64_Sxword r_addend;
}Elf64_Rela
</code></pre><p>这里，<code>r_offset</code>是重定位入口，相对于段起始的偏移；<code>r_info</code>则是重定位入口的类型和符号。其低位表示重定位入口的类型；高位表示重定位入口的符号，在符号表中的下标。（不同处理器的格式不一样）<br>符号解析则是为符号的重定位提供帮助，根据多个目标文件中的符号表，生成全局符号表，找到相应的符号并进行重定位。对于未定义的符号，链接器都应该能在全局符号表中找到，否则就报出符号未定义的错误。<br>PS：x86_64只使用Elf64_Rela。  </p>
<h4 id="指令修正"><a href="#指令修正" class="headerlink" title="指令修正"></a>指令修正</h4><p>在x86_64中，call、jmp、mov、lea等指令的寻址方式千差万别。对于重定位来说，修正指令的寻址方式定义在binutils/elfcpp/x86_64.h当中。这其中主要包括：<code>R_X86_64_64</code>和<code>R_X86_64_PC32</code>两种。这是因为X86_64上，相对寻址依然只支持32位（实际上这也很科学；因为一个可执行文件通常不会有4G那么大）。<br>两种寻址方式的修正方法分别为：<code>符号地址 + 保存在被修正位置的值</code>和<code>符号地址 + 保存在被修正位置的值  - 被修正的位置相对于段开始的偏移量</code>  </p>
<h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><h6 id="第一步：初始化，parsing-command-line-amp-script-file"><a href="#第一步：初始化，parsing-command-line-amp-script-file" class="headerlink" title="第一步：初始化，parsing command line &amp; script file"></a>第一步：初始化，parsing command line &amp; script file</h6><p>linker的入口，在ldmain.c当中(通常在链接的时候，通过编译器内部直接进行调用)。首先，linker会调用bfd库，识别二进制文件的格式，生成各个段的描述符，并且转换为<code>canonical form</code>。（例如linker中的符号表识别工作，就是首先由BFD来进行分析和转化，然后linker直接在<code>canonical form</code>上进行操作，再由BFD来进行输出）因此在ldmain.c的<code>main</code>中，首先进行的也是bfd的初始化<code>bfd_init</code>。随后linker进行了一系列的设置，包括路径，回调函数、初始化，加载插件、读取命令、linker script等。  </p>
<h6 id="第二步：文件和符号的加载"><a href="#第二步：文件和符号的加载" class="headerlink" title="第二步：文件和符号的加载"></a>第二步：文件和符号的加载</h6><p>随后，<code>lang_process</code>中，linker会对每个输入文件进行处理。对于每个输入文件，linker都会分配一个bfd，对输入文件进行扫描，识别出其中的符号。首先<code>open_input_bfds</code>为所有文件建立了bfd，随后载入文件中的所有符号。每个符号对应一个<code>bfd_link_hash_entry</code>，它们保存在<code>bfd_link_hash_table</code>当中。<code>bfd_link_add_symbols</code>将符号添加到hash_table当中。  </p>
<h6 id="第三步：输入文件的分析和合并"><a href="#第三步：输入文件的分析和合并" class="headerlink" title="第三步：输入文件的分析和合并"></a>第三步：输入文件的分析和合并</h6><p>在链接的第一部分完成后，第二部分开始前，链接器首先调用了<code>ldctor_build_sets</code>函数，它主要为C++中的constructor/dectructor提供支持。随后链接器<code>lang_do_memory_region</code>计算出内存区域（它们保存在<code>lang_memory_region_list</code>当中）。再通过<code>lang_common</code>处理全局符号，将它们添加到对应的section，移除没有被使用的sections等。随后链接器建立输入section和输出section之间的映射关系，并且将文件的section合并，以及设置段的属性等。  </p>
<h6 id="第四步：重定位"><a href="#第四步：重定位" class="headerlink" title="第四步：重定位"></a>第四步：重定位</h6><p>第四步是符号的重定位工作。这里<code>lang_size_section</code>首先获取所有section的大小，然后<code>lang_set_startof</code>会修正section的大小和位置。在确定了sections的信息之后，就可以对符号进行重定位了，这便是<code>lang_do_assignments</code>和<code>ldexp_finalize_syms</code>的工作。它们会按照前面提到的方法，对符号进行重定位。最后链接器还会检查符号和section的正确性。  </p>
<h6 id="第五步：交给bfd，输出文件"><a href="#第五步：交给bfd，输出文件" class="headerlink" title="第五步：交给bfd，输出文件"></a>第五步：交给bfd，输出文件</h6><p>在完成重定位之后，如果没有出现异常，linker就把工作交给bfd了。<code>ldwrite</code>负责把链接好的文件输出。完成一些清理工作后，整个链接过程就结束了。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;GCC的工作，到生成汇编代码为止。剩下的工作，交给了Binutils来完成：assembler和static linker。最近详细地研究了一下linker的工作过程。&lt;br&gt;linker主要完成的是静态链接，目标文件合并的工作。例如，把多个.o文件合并成一个可执行文件。 
    
    </summary>
    
    
      <category term="链接器" scheme="http://sec-lbx.tk/tags/%E9%93%BE%E6%8E%A5%E5%99%A8/"/>
    
      <category term="binutils" scheme="http://sec-lbx.tk/tags/binutils/"/>
    
  </entry>
  
  <entry>
    <title>Hack GCC，在编译时构造新的函数</title>
    <link href="http://sec-lbx.tk/2017/06/09/Hack%20gcc%EF%BC%9A%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%9A%84%E5%87%BD%E6%95%B0/"/>
    <id>http://sec-lbx.tk/2017/06/09/Hack gcc：添加新的函数/</id>
    <published>2017-06-09T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="怎么做和为什么？"><a href="#怎么做和为什么？" class="headerlink" title="怎么做和为什么？"></a>怎么做和为什么？</h4><p>首先对于GCC来说，<strong>添加一个新的函数</strong>是完全可以实现的，但我并没有看到任何相关的资料。我给gcc的mailing list发了封邮件，但没人给出回答。倒是有人私下给我发了封邮件，表示他对这个做法很好奇，希望我做出来之后能和大家分享一下方法。<br>看来求人终究不如求自己，我看了好几天代码，终于能够在编译的过程中添加函数了。<br>至于为什么要这么做，我认为如果编译时<strong>插桩的代码是大量重复的</strong>，那么去构造一个函数，并且插桩对这个函数的调用，是能够减少最后编译的可执行文件的体积的。  </p>
<h4 id="从函数的结构出发"><a href="#从函数的结构出发" class="headerlink" title="从函数的结构出发"></a>从函数的结构出发</h4><p>要构造一个新的函数，不妨先看看函数包含有哪些部分：<a href="https://gcc.gnu.org/onlinedocs/gccint/Function-Basics.html#Function-Basics" target="_blank" rel="external">这里</a>说明了函数的四个核心部分：函数名、参数、返回值、函数体。那么生成一个函数，也就要构造这些部分；并且对于函数的某些property，也应该进行相应的设置。  </p>
<p>有兴趣的朋友可以参考这几个函数：tree-parloops.c中的<code>create_loop_fn()</code>，omp-low.c当中的<code>create_omp_child_function()</code>，以及cgraphunit中的<code>init_lowered_empty_function</code>。  </p>
<h4 id="函数的构造"><a href="#函数的构造" class="headerlink" title="函数的构造"></a>函数的构造</h4><p>总的来说，如果要构造一个函数，需要完成这些步骤：</p>
<ol>
<li>构造函数的type</li>
<li>构造函数的name</li>
<li>构造函数的declaration</li>
<li>构造函数体</li>
<li>创建函数的cgraph_node</li>
<li>设置函数的attributes</li>
<li>构造函数的result</li>
<li>构造函数的paramater</li>
<li>添加新的函数</li>
</ol>
<h5 id="type-amp-name"><a href="#type-amp-name" class="headerlink" title="type&amp;name"></a>type&amp;name</h5><p>build_function_type(tree return_type, …)`用来构造一个function的type。return_type是函数返回的类型。可变参数是用来设置额外的参数类型的，参数类型必须由NULL_TREE来终结。  </p>
<p>那么这个这些type是从哪儿来的呢？<code>build_complex_type</code>用来生成一系列组合的type，比如<code>unsigned long</code>，而其他的基本类型实际上已经定义好了，例如<code>void_type_node</code>，<code>ptr_type_node</code>等。可见，数据的类型，就是通过这种方式来定义的，对于用户自己定义的数据类型，同样会通过build_complex_type来处理。  </p>
<p><code>get_identifier</code>会返回一个标识符id，如果name能够找到，它会返回原有值，如果找不到，就会创建一个新的表项并返回。随后<code>decl</code>会实际去构造一个声明。<code>build_decl</code>的内部如下： </p>
<pre><code>tree
build_decl_stat(location_t loc, enum tree_code code, tree name, tree type MEM_STAT_DECL){
tree t;
t = make_node_stat(code PASS_MEM_STAT);
</code></pre><h5 id="declaration"><a href="#declaration" class="headerlink" title="declaration"></a>declaration</h5><p><code>build_fn_decl(const char *name, tree type)</code>函数用来构造并且返回一个函数的声明。这里详细看看这个函数的内部：</p>
<pre><code>tree build_fn_decl(const char *name, tree type){
    tree id = get_identifier(name);
    tree decl = build_decl(input_location, FUNCTION_DECL, id, type);
    DECL_EXTERNAL(decl) = 1;
    TREE_PUBLIC(decl) = 1;
    DECL_ARTIFICIAL(decl) = 1;
    TREE_NOTHROW(decl) = 1;
}
</code></pre><h5 id="函数体"><a href="#函数体" class="headerlink" title="函数体"></a>函数体</h5><p>然而，这里仅仅是构造了一个声明，对于一个函数来说，不仅应该有它的声明，还应该有其函数体。<code>allocate_struct_function(tree fndecl, bool abstract_p)</code>会为一个声明生成一个function structure，并且把它设置为默认内容。<code>abstract_p</code>表示这个函数是一个抽象的(类似于函数模版)。<br>在创建函数体之后，还要创建函数体的基本块：赋予函数最基本的结构，并且正确的连接ENTRY_BLOCK和EXIT_BLOCK。这个步骤可以通过<code>create_basic_block</code>来完成。  </p>
<h5 id="cgraph-node"><a href="#cgraph-node" class="headerlink" title="cgraph_node"></a>cgraph_node</h5><p><code>cgraph_node</code>定义在cgraph.h中，它是一个<code>symtab_node</code>的子类，也就是符号表的子类。它包含有函数声明的调用关系（这也是它的名字call graph node的由来），也就是<code>callee</code>和<code>caller</code>。<br><code>cgraph_node::get_create()</code>能够创建一个函数的call graph。这个函数会为某个函数声明，找到对应的的call graph。如果不存在这样的call graph，就为它构造一个call graph。<br>这个过程还将函数加入符号表<code>symtab</code>，这是极其重要的。函数间调用的转换，也依赖于这个符号表。  </p>
<h5 id="attributes"><a href="#attributes" class="headerlink" title="attributes"></a>attributes</h5><p><code>tree_cons</code>用来创建TREE_LIST结点。它根据purpose、value（实际上也就是tree_list的两个元素），创建一个结点，并设置这个node的tree_chain。对于函数来说，它的attributes通常是保存在一个TREE_LIST当中的。  </p>
<h5 id="paramaters-amp-result"><a href="#paramaters-amp-result" class="headerlink" title="paramaters&amp;result"></a>paramaters&amp;result</h5><p>对一个函数来说，它的参数和返回值，同样都是利用tree结构来表示的。对于一个函数，参数是可以不设置的，但返回值不行。这两者可以通过这样的形式来构造声明：  </p>
<pre><code>t = build_decl(UNKNOWN_LOCATION, RESULT_DECL, NULL_TREE, void_type_node);
DECL_CONTEXT(t) = fndecl;
DECL_RESULT(fndecl) = t;
t = build_decl(UNKNOWN_LOCATION, PARAM_DECL, NULL_TREE, void_type_node);
DECL_CONTEXT(t) = fndecl;
DECL_ARGUMENTS(fndecl) = t;
</code></pre><p>当然，还要设置这它们和函数之间的上下文关系。  </p>
<h5 id="添加新的函数"><a href="#添加新的函数" class="headerlink" title="添加新的函数"></a>添加新的函数</h5><p>cgraphunit.c当中，提供了一个函数<code>cgraph_node::add_new_function</code>。在编译过程中，编译器会插入一些新的函数，这个函数将新的函数添加到一个数组<code>cgraph_new_nodes</code>当中去。这个函数还会对所添加的函数，执行之前所有“错过”的pass。因此不论在任何时间点插入函数，其所经历的优化过程其实并没有减少；值得一提的是，这个函数只允许插入GIMPLE形式的函数（high，low，ssa），所以我想<a href="https://gcc.gnu.org/ml/gcc/2003-10/msg00330.html" target="_blank" rel="external">这个问题</a>目前应该是不能实现的；因为如果直接写RTL，可能会无法恢复成GIMPLE，也就没法去进行某些优化了。<br><code>cgraph_new_nodes</code>中所保存的函数，会被<code>process_new_functions</code>处理，将它们添加到call graph当中去。现在这些函数就像原有的函数一样了。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;怎么做和为什么？&quot;&gt;&lt;a href=&quot;#怎么做和为什么？&quot; class=&quot;headerlink&quot; title=&quot;怎么做和为什么？&quot;&gt;&lt;/a&gt;怎么做和为什么？&lt;/h4&gt;&lt;p&gt;首先对于GCC来说，&lt;strong&gt;添加一个新的函数&lt;/strong&gt;是完全可以实现的，但我
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>Hack GCC，在RTL上插桩</title>
    <link href="http://sec-lbx.tk/2017/05/16/Hack%20GCC%EF%BC%9Ainsturment%20on%20RTL/"/>
    <id>http://sec-lbx.tk/2017/05/16/Hack GCC：insturment on RTL/</id>
    <published>2017-05-16T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="insn的创建"><a href="#insn的创建" class="headerlink" title="insn的创建"></a>insn的创建</h4><p>在<code>emit-rtl.c</code>当中，列举了一系列的<code>emit_insn</code>函数：  </p>
<pre><code>/* 在before前插入x */
emit_insn_before_noloc(rtx x, rtx before)
/* 在after后插入x */
emit_insn_after_noloc(rtx x, rtx after)
/* 插入x的同时插入jump */
emit_jump_insn_before(rtx x, rtx before)
emit_jump_insn_after(rtx x, rtx after)
/* 插入x的同时插入call */
emit_call_insn_before(rtx x, rtx before)
emit_call_insn_after(rtx x, rtx after)
</code></pre><p>emit，也即发射。在指令生成之后，可以利用这一系列的API，把它“发射”到指令序列当中去。首先是要生成相应的指令，包括。其实insn-emit当中，已经提供了许多模版，比如<code>gen_nop</code>，就可以生成一条nop指令。    </p>
<h4 id="instrument-bound-check"><a href="#instrument-bound-check" class="headerlink" title="instrument bound check"></a>instrument bound check</h4><p>在gcc/config/i386/i386.c当中，<code>ix86_expand_builtin</code>函数包含了一系列“处理器内置函数”的expand过程。这个函数中，首先用<code>DECL_FUNCTION_CODE()</code>对对应的内置功能进行判断，随后根据内置功能的类型进行进一步判断。其中，<code>IX86_BUILTIN_BNDCU/IX86_BUILTIN_BNDCL</code>就是bound check指令对应的处理。不妨来看看这两类指令是如何生成的吧：</p>
<pre><code>case IX86_BUILTIN_BNDCU:
arg0 = CALL_EXPR_ARG(exp, 0);
arg1 = CALL_EXPR_ARG(exp, 1);
op0 = expand_normal(arg0);
op1 = expand_normal(arg1);
if(!register_operand(op0, Pmode))
    op0 = ix86_zero_expand_to_Pmode(op0);
if(!register_operand(op1, BNDmode))
    op1 = copy_to_mode_reg(BNDmode, op1);
emit_insn(BNDmode == BND64mode ? gen_bnd64_cu(op1, op0):gen_bnd32_cu(op1, op0));
</code></pre><p>可见,<code>arg0</code>和<code>arg1</code>是由<code>exp</code>得到的，随后被转化为<code>op0</code>和<code>op1</code>。这里对于<code>op0</code>和<code>op1</code>的模式进行了判断。这里的判断，是对操作数的machine mode进行了判断。对于<code>op0</code>来说，是判断它是否满足<code>Pmode</code>，也即指针使用的模式（比如x86_64用了48bit，这里的P表示partial）；而BNDmode则表示的是指针”bounds”的模式。后面的<code>gen_bnd64_cu(op1, op0)</code>也表明，<code>op1</code>是指针指向的地址，<code>op0</code>是bound寄存器的值。  </p>
<h4 id="寄存器的申请"><a href="#寄存器的申请" class="headerlink" title="寄存器的申请"></a>寄存器的申请</h4><p>对于<code>op1</code>来说，其值也就是相应的指令进行访问的地址。而<code>op0</code>则是具体的寄存器了，那么这个寄存器如何申请呢？在RTL当中，寄存器的选择是由<code>gen_rtx_REG()</code>来决定的。它的第一个参数，选择了这个寄存器的mode，第二个参数则是通过一个编号，来选择寄存器。这些寄存器的编号，定义在gcc/config/i386/i386.h当中。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;insn的创建&quot;&gt;&lt;a href=&quot;#insn的创建&quot; class=&quot;headerlink&quot; title=&quot;insn的创建&quot;&gt;&lt;/a&gt;insn的创建&lt;/h4&gt;&lt;p&gt;在&lt;code&gt;emit-rtl.c&lt;/code&gt;当中，列举了一系列的&lt;code&gt;emit_insn&lt;
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>Hack GCC，改变函数的顺序</title>
    <link href="http://sec-lbx.tk/2017/05/12/Hack%20GCC%EF%BC%8C%E4%BF%AE%E6%94%B9%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E4%B8%AD%E5%87%BD%E6%95%B0%E7%9A%84%E9%A1%BA%E5%BA%8F/"/>
    <id>http://sec-lbx.tk/2017/05/12/Hack GCC，修改二进制文件中函数的顺序/</id>
    <published>2017-05-12T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="GCC的函数输出"><a href="#GCC的函数输出" class="headerlink" title="GCC的函数输出"></a>GCC的函数输出</h4><p>直观的来看，在<code>cgraphunit.c</code>当中，<code>expand_all_functions()</code>被用来输出所有需要输出的函数。这里，它首先进行了一次拓扑排序，在<code>ipa_reverse_postorder()</code>中完成。这样做的好处是，当一个函数被输出时，它所调用的所有函数都已经被输出了。这个函数还会先把内联的情况给处理掉。<br>在<code>expand_all_functions()</code>中，随后在允许函数重排列的情况下，利用<code>qsort</code>，对其顺序进行了重新排列。其比较函数，是通过比较两个<code>cgraph_node</code>代表的函数第一次执行的时间来进行的。<br>奇怪的是，在我修改了<code>expand_all_functions()</code>中的order之后，输出文件的格式并没有变。这里我出了一点小差错，结果在编译GCC的时候出现了问题，不过这也恰好说明GCC是通过自举来完成编译的。<br>于是我继续在上一层函数进行查找，在<code>symbol_table::compile</code>当中，实际上通过了了一个<code>flag_toplevel_reorder</code>来进行判断。在该值为false时，函数的调用会直接调用：<code>output_in_order(false)</code>。在实验之后，我发现gcc默认使用的是<code>-fno-toplevel-reorder</code>的方式，也即使用的是<code>output_in_order(false)</code>；但它自举的时候使用的是<code>-ftoplevel-reorder</code>的方式，也即<code>output_in_order(ture)</code>。这很自然：gcc优化自己的代码，这是它自身的需要。</p>
<pre><code>if(!flag_toplevel_reorder)
    output_in_order(false);
else{
    output_in_order(true);
    expand_all_functions();
    output_variables();
}
</code></pre><p>那么不妨看看<code>output_in_order()</code>中的代码。</p>
<pre><code>FOR_EACH_DEFINED_FUNCTION(pf){
    ...
    if(no_reorder &amp;&amp; !pf-&gt;no_reorder)
    continue;
    i = pf-&gt;order;
    nodes[i].kind = ORDER_FUNCTION;
    nodes[i].u.f = pf;
}
</code></pre><p>很明显，如果采用reorder的方式，也即output_in_order(ture)，那么这里会跳过nodes的排序部分，并且进一步执行<code>expand_all_functions()</code>。<br>而如果采用的是no_reorder，那么nodes[i]所对应的也就是通过FOR_EACH_DEFINED_FUNCTION遍历到的第i个函数了，也就是严格按照次序来决定的。</p>
<h4 id="改变函数的输出次序"><a href="#改变函数的输出次序" class="headerlink" title="改变函数的输出次序"></a>改变函数的输出次序</h4><p>那么如果想要更改汇编输出的函数次序，对于优化的情况，在<code>expand_all_functions()</code>中，在<code>qsort</code>对函数进行重排列之后，对order进行修改就可以了。那么GCC自身为什么要改变函数的顺序呢？因为把具有调用关系的函数，放在同一个页当中，是可以有效的减少缺页的情况的。不过现在内存的大小都很大，其实代码页并不会占据很多内存，所以从这个角度上来看，其实差异没有想象中那么大。另一种方式是采用和没有优化的方式相同的处理方法，但是在编译时加上<code>-fno-toplevel-reorder</code>选项。<br>而假如没有进行优化，那么直接在<code>output_in_order()</code>当中，对nodes中的次序进行修改就行了。那么函数的随机化就可以在这些知识的基础之上实现了。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;GCC的函数输出&quot;&gt;&lt;a href=&quot;#GCC的函数输出&quot; class=&quot;headerlink&quot; title=&quot;GCC的函数输出&quot;&gt;&lt;/a&gt;GCC的函数输出&lt;/h4&gt;&lt;p&gt;直观的来看，在&lt;code&gt;cgraphunit.c&lt;/code&gt;当中，&lt;code&gt;expan
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>RTL，指令和运算，识别特定指令</title>
    <link href="http://sec-lbx.tk/2017/05/10/RTL%E8%AF%A6%E8%A7%A3/"/>
    <id>http://sec-lbx.tk/2017/05/10/RTL详解/</id>
    <published>2017-05-10T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h4><p>RTL中的运算，采用一种很直观、简单的方式来进行描述。通常，表达式都会借助于m mode。运算包含有算术、比较、向量操作、Bit域、类型转换等。  </p>
<h4 id="Side-Effect-表达式"><a href="#Side-Effect-表达式" class="headerlink" title="Side Effect 表达式"></a>Side Effect 表达式</h4><p>之前所说的表达式代码，通常表示的是值，而非某种行为。但机器指令，只有在对机器的状态产生了改变时，才会有意义；因此对于这些对寄存器、执行状态产生影响的过程，是有对应的表达式的，被称为side effect表达式。<br>一条指令的主体，一定是这些side effect之一；而之前表示值的表达式代码，只是作为它们的操作数出现的。<br>set lval x：表示将x的止保存在lval当中。lval表示一个能够存放值的位置。它们包括（仅列出了比较常用的一部分）：<br>return/simple_return：表述函数返回。<br>call function nargs：表示函数调用。<br>clobber x，表示不可预期的，可能的存储，将不可描述的值保存到x，必须为一个<code>reg</code>，<code>scratch</code>，<code>parallel</code>或者<code>mem</code>。说明x的值可能被修改（也就是说值会被改变？）<br>use x：x的值被使用。x必须为寄存器。<br>parallel：并行发生的side effects。<br>cond_exec [cond expr]：条件执行的表达式。<br>sequence [insns]：顺序执行的insn序列。<br>除此之外，还有一些和内存地址有关的side effects，例如pre_dec:m x表示x减少一个标准值，其中m必须是指针的对应machine mode。例如(mem:DF (pre_dec:SI (reg:SI 39)))表示，减少伪寄存器39的值(减少DFMODE的大小)并且把结果作为一个DFMODE值。  </p>
<h4 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h4><p>一个函数的代码的RTL形式，是保存在一个双向链表当中的，它被称为insns。insn是具有特殊代码的表达式，它们不作它用，其中一部分是用来生成真实的代码的，另一部分表示jump table，或者jump的labels。<br>insn除了自身特定的数据之外，每一个insn还必须有一个独有的id－number，用来和当前函数中其他的insns进行区分，并且和其他的insns进行串联。在每个insns中，都有这样三个区域，分别用INSN_UID，PREV_INSN，NEXT_INSN这三个宏定义来取值。<br>insn一定包含有如下的某一类表达式代码：<br>insn：非jump/call的代码。<br>jump_insn：可能会造成跳转的指令，包括ret。<br>call_insn：可能进行函数调用的指令。call_insn也包含有一个域，CALL_INSN_FUNCTION_USAGE，它包含有一个链表，它记录了call调用所调用函数所需要利用的寄存器、内存地址等。<br>code_label：表示jump_insn能够跳转到的label。它包含两个特殊的域，分别是CODE_LABEL_NUMBER和LABEL_NUSES，后者只在jump优化阶段结束之后，它包含目前这个函数中，该label被使用了多少次。<br>jump_table_data：用来保存jumptable，位于tablejump_p insn之后。<br>barrier：放在非条件jump指令之后。<br>note：表示额外的调试信息。<br>debug_insn：用来track的伪代码。  </p>
<h4 id="insn，jump-insn-call-insn"><a href="#insn，jump-insn-call-insn" class="headerlink" title="insn，jump_insn,call_insn"></a>insn，jump_insn,call_insn</h4><p>这几类指令都含有额外的域：<br>PATTERN(i)：这条指令的side effect表达式。<br>INSN_CODE(i)：一个integer，表示了这个insn机器描述的pattern，但通常一般不会做匹配，因此一般都是－1，尤其是对于use，clobber，asm_inpu，addr_vec和addr_diff_vec表达式而言。<br>LOG_LINK：insn_list链表，记录了instructions和basic block的依赖关系。只被schedulers和combine使用。<br>REG_NOTES：一个链表，记录了和寄存器有关的信息。具体的，宏REG_NOTE_KIND返回了register note的类型。而PUT_REG_NOTE_KIND则用来修改类型。  </p>
<h4 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h4><p>那么，不妨来看看在gcc里面，一个insn到底是如何表示的：</p>
<pre><code>(insn 21
      20
      22
      6 
      (set (reg:SI 59 [ D.1238 ])
           (mem/c/i:SI (plus:SI (reg/f:SI 54 virtual-stack-vars)
                                (const_int -8 [0xfffffffffffffff8])) [0 sum+0 S4 A32]))
      /home/kito/sum.c:7
      -1
      (nil))
</code></pre><p>把它和”iuuBeiie”对应起来，那么其实也就是一对一的关系，这里解释的很到位：  </p>
<pre><code>(insn 21 # 1. i
      20 # 2. u
      22 # 3. u
      6  # 4. B
      (set (reg:SI 59 [ D.1238 ])
           (mem/c/i:SI (plus:SI (reg/f:SI 54 virtual-stack-vars)
                                (const_int -8 [0xfffffffffffffff8])) [0 sum+0 S4 A32])) # 5. e
      /home/kito/sum.c:7 # i
      -1 # 6. i
      (nil)) # 7. e
</code></pre><p>i 代表该 INSN 的 uid<br>u 上一道 INSN 的 uid<br>u 下一道 INSN 的 uid<br>B Basic Block的 编号<br>e 该 INSN 的主要內容, 例如上面那道指令所描述的是從内存读取一個值到寄存器中<br>i 该 INSN 对应源码的位置<br>i 放 RTL pattern Name<br>e 放 REG_NOTES 主要和寄存器有关   </p>
<p>其中，这个<code>“iuuBeiie”</code>是打印的格式，每个字符的含义定义在rtl.c当中。这里，我比较关心的’e’指的是一个rtl表达式。再来看一个例子。call_insn对应的输出是<code>“iuuBeiiee”</code>，其输出为：</p>
<pre><code>(call_insn    16 #1. i
            15 #2. u
            17 #3. u
            2  #4. B
            (call (mem:QI (symbol_ref:DI (&quot;printf&quot;)[flags 0x41] &lt;function_decl 0x7f1fd042e100 printf&gt;)[0 __builtin_printf S1 A8]) const_int 0 [0])))) #5. e
            hello.c:12 #6. i
            649     #7. i
            (nil)    #8. i
            (expr_list (use (reg:QI 0 ax))
                (expr_list:DI (use (reg:DI 5 di))
                    (expr_list:SI (use (reg:SI 4 si))
                        (nil)))))    #9. e
)
</code></pre><p>和普通的insn相比，多出来的一项expr_list，是一个链表，它包含了用来传递参数的寄存器、内存的信息。  </p>
<h4 id="RTL中的寄存器"><a href="#RTL中的寄存器" class="headerlink" title="RTL中的寄存器"></a>RTL中的寄存器</h4><p>从GCC官方的说明来看，RTL和llvm一样，首先是假设“有无限个寄存器”的，这也就是说，在RTL代码中，必然会有伪寄存器存在。在编译过程中，这些伪寄存器，要么被替换为真实的、硬件寄存器，要么被替换为内存引用。我也为此困扰过：那么在翻译时，是如何确定RTL具体对应的机器码的呢？RTL既然是一个和机器相关的语言，那么寄存器的选择，应该与最后的指令生成无关了。<br>其实这个问题是在gcc内置的rtl pass当中解决了。具体的，<code>register alloction</code>这个（准确的说是一系列）pass，它保证了所有的伪寄存器被清除了，其方式正是将它们替换为硬件寄存器、常量或者栈上的值。这也就是说，假设想要做一些和寄存器的选择相关的优化，最好是在这个过程之后去做。<br>在这个过程之后，rtx中的<code>reg</code>就表示的是实实在在的寄存器了。而相对的，<code>subregs</code>也不存在了，而<code>mem</code>毫无疑问也就表示了对表达式addr所示地址的主内存进行引用。其中<code>mem:m addr alias</code>中，m描述的是被访问内存单元的大小，alias代表这个引用点别名集合，只有引用相同内存地址的项，才会放在一个别名集合里面。  </p>
<h4 id="RTL，读取内存"><a href="#RTL，读取内存" class="headerlink" title="RTL，读取内存"></a>RTL，读取内存</h4><p>既然知道<code>mem:m addr alias</code>用来引用内存，那么<code>addr</code>这个东西到底是怎么表示的？它是用一个寄存器来保存指针吗？还是用一个变量名称呢？首先我们来看看有哪些表达式会读取内存吧：<br><strong>副作用表达式</strong><br><code>set lval x</code>，表示将x对值放到由lval表示的地方。那么如果x是一个<code>mem</code>，无疑是会对内存进行引用的。<br><code>parallel [x0 x1 ...]</code>，多个并行执行的副作用，那么其中的每一个<code>set</code>都会发生内存的引用。<br><strong>算术表达式</strong><br>在RTL中，算术表达式并不会改变一个值，它只是一个单独的运算，代表一个结果。举个例子，如果是一个<code>ADD</code>指令，那么它实际上在RTL中是通过一个parallel来表示，这个parallel包括了<code>plus</code>，还包含了<code>set</code>，只不过在汇编阶段生成的只是一条指令而已。所以说其实RTL里面的parallel，并不是说的概念上的并行，而是说某些表达式，会同时发生、生效。<br>在<code>mem:m addr alias</code>当中，mem既可以是寄存器中存储的地址，也可以是常量指针中的地址。  </p>
<h4 id="指令的遍历"><a href="#指令的遍历" class="headerlink" title="指令的遍历"></a>指令的遍历</h4><p>在GCC，所有的pass，都是以函数为粒度来进行调用的。它是一种“run on function”的机制。在gcc内部，当前处理的函数用<code>cfun</code>来进行访问。<br>而指令，是由特殊的rtx表达式：<code>insn</code>来表示的。那么如何访问每一条指令呢？我个人认为，首先遍历基本块，随后再在基本块的内部进行遍历，是更为有效的。也就是这样的方式：</p>
<pre><code>FOR_EACH_BB_FN(bb, cfun){
    FOR_BB_INSNS(bb, insn){
        ...    //handle insn
    }
}
</code></pre><p>但是，INSN中其实还包含有<code>note</code>，如果只想留下<code>insn</code>，那么用<code>INSN_P(insn)</code>进行过滤就可以。这样一来，剩下的就只有<code>insn</code>、<code>jump_insn</code>、<code>call_insn</code>三种情况了（还有<code>debug_insn</code>，这里不考虑这种情况）。这三种情况，实际上还可以用宏继续细分判断，分别是<code>JUMP_P</code>，<code>CALL_P</code>，<code>NONJUMP_INSN_P</code>。  </p>
<h4 id="需要关注哪些指令？"><a href="#需要关注哪些指令？" class="headerlink" title="需要关注哪些指令？"></a>需要关注哪些指令？</h4><p>这里，我想定义的内存读取，指的是“通过一个指针，从内存中取值，并且把它放在寄存器当中”，当然这个取值可能会在计算之后放在寄存器中，例如<code>ADD</code>，也可能是通过<code>MOV</code>直接放到寄存器里面。无论如何，只要涉及到访问内容的数据（通过指针实现），并且结果被放到了寄存器中，这个行为就被认为是内存读取了。<br>当然，在RTL中，指令还没有那么的直观。那么如何识别出这些指令呢？首先，有一点是可以肯定的，那就是实际上只有<code>nonjump_insn</code>会产生这些指令。我在分析之后，发现：<code>jump_insn</code>、<code>call_insn</code>，即使是间接跳转的（比如c++里面，常常可以见到的vpointer），它们其实也只是代表控制流转移的过程而已，其指针的提取、计算，其实都是放在这条指令之前的<code>insn</code>中去执行的。  </p>
<h4 id="指令中的内存读取"><a href="#指令中的内存读取" class="headerlink" title="指令中的内存读取"></a>指令中的内存读取</h4><p>下一步，就是从这些指令中，把真正的指令读取给提取出来。对应一个insn，它的输出格式是<code>&quot;iuuBeiie&quot;</code>，其中，e也即rtx表达式。在输出的时候，这部分表示的是这个insn的实际代码。<code>GET_CODE</code>能够用来提取出insn的代码部分，其实它只是一个<code>enum</code>：<code>#define GET_CODE(RTX) ((enum rtx_code) (RTX)-&gt;code)</code>，而<code>PATTERN</code>则是获取这个insn的副作用。这里，不应该被其名字所迷惑：因为<code>GET_CODE</code>不过是获取rtx code的类型，它和这个insn的内容，反而没有联系；相反，<code>PATTERN</code>则说明了这个insn会带来什么副作用，也就是对值的影响。<br>在看了<code>var-tracking.c</code>和<code>gcse.c</code>当中的源码之后，我注意到一条<code>set</code>指令中的<code>src</code>和<code>dest</code>，实际上都可以用宏提取，也即<code>rtx src = SET_SRC(PATTERN(insn))</code>和<code>rtx dest = SET_DEST(PATTERN(insn))</code>。<br>而PATTERN的代码类型其实也很有限。在<code>var-tracking</code>之后，实际上<code>sequence</code>也已经被去除了。那么实际上我们需要关注的PATTERN也就只有两种：分别是<code>set</code>和<code>parallel</code>。（实际上还有一种，那就是<code>asm_input</code>，这里先不予考虑）<br>那么对于<code>insn</code>，也就只有两种情况，一是一个单独的<code>set</code>，二是一个<code>parallel</code>，它可能包含了很多条<code>insn</code>，那么其中的指令就要逐条分析了。<br>注意：在gcc 5.0之后点版本，专门的数据结构rtx_insn表示一条指令，而不是用rtx来一概而论了。  </p>
<h4 id="SET的分析"><a href="#SET的分析" class="headerlink" title="SET的分析"></a>SET的分析</h4><p>一个<code>set val x</code>，其lval可以是<code>reg</code>，<code>mem</code>，<code>pc</code>，<code>parallel</code>或者<code>cc0</code>。其中，<code>parallel</code>用来表示一个函数通过多个寄存器来返回一个结构体的情况，这里不用关注这个情况，只需要关注lval是<code>reg</code>，而x是<code>mem</code>的情况。（目前只考虑了这个最基本的情况）<br>对于<code>x</code>来说，set的源操作数可能是一个内存地址，还可能是一个表达式的，比如加减乘除、位操作等。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;运算&quot;&gt;&lt;a href=&quot;#运算&quot; class=&quot;headerlink&quot; title=&quot;运算&quot;&gt;&lt;/a&gt;运算&lt;/h4&gt;&lt;p&gt;RTL中的运算，采用一种很直观、简单的方式来进行描述。通常，表达式都会借助于m mode。运算包含有算术、比较、向量操作、Bit域、类型转换
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>RTL，类型和操作数</title>
    <link href="http://sec-lbx.tk/2017/05/07/gcc%20rtx/"/>
    <id>http://sec-lbx.tk/2017/05/07/gcc rtx/</id>
    <published>2017-05-07T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="RTX"><a href="#RTX" class="headerlink" title="RTX"></a>RTX</h4><p>rtx(RTL expression)，也即一个RTL的表达式。在<code>coretypes.h</code>中，其被定义为<code>struct rtx_def *rtx</code>，也即一个指向rtx_def的指针。那么rtx_def是一个什么结构呢？它同样定义在rtl.h当中：</p>
<pre><code>struct GTY((chain_next (&quot;RTX_NEXT (&amp;%h)&quot;),
    chain_prev (&quot;RTX_PREV (&amp;%h)&quot;), variable_size)) rtx_def {
  /* 表达式类型 */
  ENUM_BITFIELD(rtx_code) code: 16;

  /* 表达式包含值的类型 */
  ENUM_BITFIELD(machine_mode) mode : 8;

  /* 一系列位段，在不同情况下表示不同含义*/
  unsigned int jump : 1;

  unsigned int call : 1;

  unsigned int unchanging : 1;

  unsigned int volatil : 1;

  unsigned int in_struct : 1;

  unsigned int used : 1;

  unsigned frame_related : 1;

  unsigned return_val : 1;

  /* rtx的第一个操作数，操作数的个数和类型是在code域中定义的*/
  union u {
    rtunion fld[1];
    HOST_WIDE_INT hwint[1];
    struct block_symbol block_sym;
    struct real_value rv;
    struct fixed_value fv;
  } GTY ((special (&quot;rtx_def&quot;), desc (&quot;GET_CODE (&amp;%0)&quot;))) u;
};
</code></pre><p><a href="https://dmalcolm.fedorapeople.org/gcc/2013-10-31/doxygen/html/structrtx__def.html" target="_blank" rel="external">https://dmalcolm.fedorapeople.org/gcc/2013-10-31/doxygen/html/structrtx__def.html</a>中对每个位段进行了更详细的描述。</p>
<p>其中，rtx所包含的所有可能情况，都包含在了rtl.def当中；注意这里rtx和insn其实有一些微妙的关系：rtx表示一条中间指令的时候，它是一个insn；rtx不一定是insn，但insn一定是rtx。</p>
<h4 id="RTL：对象"><a href="#RTL：对象" class="headerlink" title="RTL：对象"></a>RTL：对象</h4><p>如果想在RTL阶段上，进行编译的优化，那么首先就必须好好了解RTL。RTL使用五种类型的对象： expressions、integers、wide integers、strings和vectors。其中，RTX是最为重要的的一个，它是一个C数据结构，通常用指针的形式来调用，也就是前面所提到的rtx。<br>interger和wide integer分别对应着机器上的整型和长整型。string则是一串非空的字符；vector则包含指向任意数量expression的指针，用[…]的形式来表示，并用空格分割。expressions被称为RTX code，它定义在rtl.def当中，其含义是与机器无关的。一个rtx的code部分，可以用<code>GET_CODE(x)</code>来获取，并且可以用<code>PUTCODE(x, newcode)</code>来替换。<br>expression code决定了：表达式所包含的操作数个数、它们是什么类型的objdect。RTL不像Lisp，不能通过operand自身来确定它是什么类型的object，而必须知道它的上下文（例如一个subreg code，它的第一个操作数被当成表达式，而第二个被认为是一个整型；又例如plus code，两个operands都被当成了表达式）。expression写作：()，包含表达式类型、flags、机器码等，以及用空格分割的操作数。  </p>
<h4 id="RTL：类和格式"><a href="#RTL：类和格式" class="headerlink" title="RTL：类和格式"></a>RTL：类和格式</h4><p>expression code可被归为许多类，它们用<strong>一个字符</strong>来表示，<code>GET_RTX_CLASS(code)</code>能够用来获取RTX代码的类型，这些类定义在<code>rtl.def</code>当中。<br>RTX_OBJ：表示一个具体的object，例如REG/MEM/SYMBOL等<br>RTX_CONST_OBJ：表示一个常量object<br>RTX_COMPARE：表示不对称的比较，例如GEU/LT。<br>RTX_COMM_COMPARE：表示对称的比较。(表示满足交换律的)<br>RTX_UNARY：一元的算数操作，例如NEG、NOT、ABS、类型转换等<br>RTX_COMM_ARITH：满足交换律的二进制操作，例如PLUS、AND。<br>RTX_BIN_ARITH：不满足交换律的二进制操作，例如MINUX、DIV。<br>RTX_BITFIELD_OPS：位操作，包含ZERO_EXTRACT，SIGH_EXTRACT。<br>RTX_TERBARY：其他有三个输入的操作，例如IF_THEN_ELSE。<br>RTX_INSN：表示整条INSN，包括INSN、JUMP_INSN、CALL_INSN。<br>RTX_MATCH：表示insns当中的matches，例如MATCH_DUP。<br>RTX_AUTOINC：表示寻址时的自增，例如POST_INC。<br>RTX_EXTRA：所有其它的codes。<br>这些class，能够让expression code采用很方便的方式来表示，比如subreg直接可以表示为’ei’。<br><code>GET_RTX_LENGTH(code)</code>和<code>GET_RTX_FORMAT(code)</code>分别会返回操作数的数量，以及格式(简写，例如一个比较的操作写成ee)  </p>
<h4 id="访问操作数"><a href="#访问操作数" class="headerlink" title="访问操作数"></a>访问操作数</h4><p>通过宏定义，可以访问一个表达式的操作数，包含<code>XEXP</code>、<code>XINT</code>、<code>XWINT</code>、<code>XSTR</code>。这些宏都包含了两个参数：其一是一个RTX，其二是从0开始数的操作数编号。例如<code>XEXP(x,2)</code>访问了x的第二个操作数(作为expression来访问)。<code>XINT(x,2)</code>访问了x的第二个操作数(作为integer来访问)。<br>使用者必须自己根据expression code来判断每个操作数是什么类型的。而对vector的访问则更复杂，<code>XVEC</code>能够获取vector指针，而<code>XVECEXP</code>和<code>XVECLEN</code>可以用来访问一个vector的元素和长度。<br>对于某些的RTL节点，还有一些特殊的操作数访问方式。例如MEM、REG、SYMBOL_REF都能够用来获取不同的相关信息。</p>
<h4 id="Machine-Modes"><a href="#Machine-Modes" class="headerlink" title="Machine Modes"></a>Machine Modes</h4><p>Machine Modes描述了数据对象的大小，以及它的表现形式。Machine mode以枚举的形式来表示，machine_mode，定义在machmode.def当中。每个RTL表达式，都包含有一个machine mode的区域。在dump文件、机器描述中，RTL的machine mode被写在表达式的后面，用一个冒号分隔。<br>在GCC中，insn相当于rtx的组合。例如，</p>
<pre><code>insn: mov ax, 8
rtx: ax
rtx: 8
rtx: mov ax, 8
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;RTX&quot;&gt;&lt;a href=&quot;#RTX&quot; class=&quot;headerlink&quot; title=&quot;RTX&quot;&gt;&lt;/a&gt;RTX&lt;/h4&gt;&lt;p&gt;rtx(RTL expression)，也即一个RTL的表达式。在&lt;code&gt;coretypes.h&lt;/code&gt;中，其被定义为&lt;c
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>GCC插件：注册、编译和使用</title>
    <link href="http://sec-lbx.tk/2017/05/05/GCC%E6%8F%92%E4%BB%B6%E7%BC%96%E5%86%99/"/>
    <id>http://sec-lbx.tk/2017/05/05/GCC插件编写/</id>
    <published>2017-05-05T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="GCC插件-注册、编译、和使用"><a href="#GCC插件-注册、编译、和使用" class="headerlink" title="GCC插件:注册、编译、和使用"></a>GCC插件:注册、编译、和使用</h4><p>GCC 4.5版本后，为用户提供了接口，允许用户去编写额外的<em>代码优化过程</em>、<em>改造代码形式</em>、<em>对代码进行分析</em>。在GCC中，插件是以共享库的形式工作的。在安装好GCC之后，如果想编写插件，首先可以确定API的位置：</p>
<pre><code>gcc -print-file-name=plugin
</code></pre><p>在gcc-plugin.h中，提供了以下的结构：<br><code>struct plugin_name_args</code>包含了GCC调用插件所需要的信息，<code>plugin_info</code>是插件的帮助信息，<code>plugin_gcc_version</code>包含了插件支持的GCC版本。  </p>
<p>对于一个插件，首先要决定它是在编译的那个阶段执行的，并且通过注册回调的方式，在编译的特殊时间段调用这个插件。GCC中的插件可以在GIMPLE、IPA、RTL等阶段工作。插件必须定义一个对应的数据结构，并且将它的信息，传递给插件框架，由它来回调插件的功能。插件所能够完成多功能，被称为”events”，它们定义在plugin.def文件当中。</p>
<p>每一个gcc插件，都需要有一个初始化模块。对于一个gimple-pass插件，其初始化数据结构为：</p>
<pre><code>static struct gimple_opt_pass myplugin_pass = 
{
    .pass.type = GIMPLE_PASS,
    .pass.name = &quot;myplugin&quot;, /* For use in the dump file */

    /* Predicate (boolean) function that gets executed before your pass.  If the
     * return value is &apos;true&apos; your pass gets executed, otherwise, the pass is
     * skipped.
     */I
    .pass.gate = myplugin_gate,  /* always returns true, see full code */
    .pass.execute = myplugin_exec, /* Your pass handler/callback */
};
</code></pre><p>在tree-pass.h中，定义了一系列的数据结构，用来设置编写pass的执行顺序。在gcc/passes.def当中，定义了所有的pass。<br><a href="http://gcc-python-plugin.readthedocs.io/en/latest/tables-of-passes.html" target="_blank" rel="external">http://gcc-python-plugin.readthedocs.io/en/latest/tables-of-passes.html</a>上标注了GCC所有的passes，以及它们的所属的阶段。我注意到，在pass编写中，是这样标注pass的位置的：</p>
<pre><code>pass.reference_pass_name = &quot;ssa&quot;;
pass.ref_pass_instance_number = 1;
pass.pos_op = PASS_POS_INSERT_AFTER;
</code></pre><p>注释中写道，这个plugin将在GCC完成SSA表现形式之后，调用这个plugin，并且在第一个SSA之后调用。从tree-pass.h中，可以看到这样的定义：</p>
<pre><code>struct register_pass_info
{
    opt_pass *pass;
    const char* reference_pass_name;/*引用新pass的原有pass的名字*/
    int ref_pass_instance_number;/*在原有pass后的特定位置插入新pass*/
    enum pass_positioning_ops pos_ops;/*具体的插入方式，替换，之前，还是之后*/
}
</code></pre><p>在pass的init过程中，还需要进行回调的注册，把这个info传递给插件框架。</p>
<pre><code>register_callback(&quot;myplugin&quot;, PLUGIN_PASS_MANAGER_SETUP, NULL, &amp;pass);
register_callback(&quot;myplugin&quot;, PLUGIN_INFO, NULL, &amp;myplugin_info);
</code></pre><p>当然，插件的运行也可以不用通过pass manager来进行hook，而是在某个特定的时间段运行。在<code>gcc/plugins.text</code>中，可以看到plugin_event的定义，它包含了一系列段时间点，于是乎在某些特定的时间点调用插件变得很方便，例如before gimplification, after compilation等。不过，如果想要更精确的设置调用的时机，那便还是要利用pass_manager来进行hook了。  </p>
<p>插件的编译与一般的共享库编译并无不同。在老版本的GCC中，插件都是用C语言编写的，然而在我看到的版本(4.9.4)中，却一律变成了C++，看来用面向对象的语言来处理还是更为方便。  </p>
<p>那么在编译好插件之后，就可以在gcc编译时调用它:</p>
<pre><code>gcc -fplugin=./myplugin.so -c -o test test1.c
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;GCC插件-注册、编译、和使用&quot;&gt;&lt;a href=&quot;#GCC插件-注册、编译、和使用&quot; class=&quot;headerlink&quot; title=&quot;GCC插件:注册、编译、和使用&quot;&gt;&lt;/a&gt;GCC插件:注册、编译、和使用&lt;/h4&gt;&lt;p&gt;GCC 4.5版本后，为用户提供了接
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>GCC，Overview</title>
    <link href="http://sec-lbx.tk/2017/05/01/GCC%20overview/"/>
    <id>http://sec-lbx.tk/2017/05/01/GCC overview/</id>
    <published>2017-05-01T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前端：AST-GENERIC"><a href="#前端：AST-GENERIC" class="headerlink" title="前端：AST/GENERIC"></a>前端：AST/GENERIC</h4><p>AST:用<code>tree</code>来表示函数。其中，<code>tree</code>实际上是一个指针，它能够指向许多种类型的<code>tree_node</code>，这些<code>tree_node</code>定义在tree-core.h当中，就不一一赘述了。只需要明白，GCC当中最重要的数据就够就是tree，如果你想要添加一种<code>tree_node</code>，也可以在tree.def里面添加。  </p>
<p>tree-core.h中定义了两种数据结构，用来直接表示tree节点，分别是tree_list和tree_vec。它们都包含有一个tree_common结构。tree_common包含有一个chain，用来和其他的tree链接起来。而tree_list则包含有purpose和value，它们都是<code>tree_node</code>。<strong>value包含了一个tree的主体</strong>。<br>可以说，在GENERIC中，<code>tree_node</code>才是主角。正如前面所说的，<code>tree_node</code>根据表示内容，也有不同的类型，而不同类型的<code>tree_node</code>的代码也不同了。比如指针类型使用POINTER_TYPE代码，而数组使用ARRAY_TYPE代码。<br>函数的声明、属性、表达式等，都是通过tree_node来表示的。在AST中，函数可以分为几个部分，分别是名字、参数、结果和函数体。这几个部分，都是通过<code>tree_node</code>来表示的。当然函数的内部还包含有许多属性。值得一提的是，有时候tree会为后端保留一些slot，用来在树被转化为RTL时，给GCC后端使用。 </p>
<h4 id="中端：GIMPLE"><a href="#中端：GIMPLE" class="headerlink" title="中端：GIMPLE"></a>中端：GIMPLE</h4><p><strong>目前C和C++的前端直接从tree转换为GIMPLE</strong>，不再先转换为GENERIC了。<br>在GCC中，<code>gimplifier</code>过程将原始的GENERIC表达式，生成（不超过3个操作数）的GIMPLE元组。GIMPLE同样是基于tree结构的中间语言。其实GIMPLE还可以进行细分，它表现出了编译器在middle end的过程。没有完全下降的GIMPLE被称为“High GIMPLE”。“Low GIMPLE”完成了进一步下降，消除了隐式的跳转和异常表达式。<br>在Low GIMPLE之后，是基于SSA的GIMPLE。SSA，“静态单赋值”保证程序中的变量，只在一个位置赋值（赋值多次就在每次赋值后产生新的变量），这样做的好处是利于数据流分析，大多数优化都是基于SSA来做的。基于GIMPLE的优化过程，都是与机器语言无关的，它包括变量的属性设置、数据流分析和优化、别名分析等。<br>目前，大多数插桩的工作，也都是在GIMPLE上完成的，因为它属于与目标机器代码无关的中间语言；这也和LLVM IR类似。  </p>
<h4 id="后端：RTL"><a href="#后端：RTL" class="headerlink" title="后端：RTL"></a>后端：RTL</h4><p>RTL则是从GIMPLE进一步拓展而来，它与机器语言直接相关。它也是大部分pass依赖的中间表示，和Lisp很类似。RTL会在优化过程中，逐渐去掉伪寄存器、合并指令，删除控制流图等。最后GCC根据RTL语言，根据insn-output当中的的模版，一一对应的输出汇编格式，再由汇编翻译成二进制代码。<br>RTL语言的定义位于rtl.def当中，在make过程中，GCC会根据机器描述文件，从对应到机器代码中提取寄存器、指令的配置，并生成相应的insn-out.c等文件，它们会在make install的时候进一步生成GCC。<br>值得一提的是，不论是RTL还是GIMPLE，都是和编译器所维护的控制流图相关的。控制流图由基本块和边来表示，并在编译过程中保持更新。控制流图在树拓展为RTL的过程中被丢弃。  </p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>总体而言，GCC的编译过程可以归纳为：从源码到语法树、从语法树到GIMPLE、从GIMPLE到RTL，再从RTL到汇编。在每个阶段，而GCC的优化工作，主要在GIMPLE和RTL上面进行。    </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前端：AST-GENERIC&quot;&gt;&lt;a href=&quot;#前端：AST-GENERIC&quot; class=&quot;headerlink&quot; title=&quot;前端：AST/GENERIC&quot;&gt;&lt;/a&gt;前端：AST/GENERIC&lt;/h4&gt;&lt;p&gt;AST:用&lt;code&gt;tree&lt;/code&gt;
    
    </summary>
    
    
      <category term="编译安全" scheme="http://sec-lbx.tk/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E5%85%A8/"/>
    
      <category term="GCC" scheme="http://sec-lbx.tk/tags/GCC/"/>
    
  </entry>
  
  <entry>
    <title>深入理解进程通信</title>
    <link href="http://sec-lbx.tk/2017/04/30/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/"/>
    <id>http://sec-lbx.tk/2017/04/30/深入理解进程通信/</id>
    <published>2017-04-30T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><p>管道是进程间的一个单向数据流；进程写入管道的数据都由内核定向到另一个进程，随后另一个进程由此从管道中读取数据，<strong>两个进程必须有亲缘关系，例如兄弟进程、父子进程</strong>。</p>
<pre><code>//经常会使用到的
$ ls | more
</code></pre><p>管道可以被视为两个“打开的文件”，但它在文件系统中没有响应映象，其原理很简单：<code>pipe()</code>系统调用会返回两个FILE文件指针，分别用于读和写，两个进程分别能够读、写管道的内容。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E7%AE%A1%E9%81%93.jpg?raw=true" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>管道的结构定义在pipe_fs_i.h中：</p>
<pre><code>struct pipe_inode_info {
    struct mutex mutex;
    wait_queue_head_t wait;    //FIFO等待队列
    unsigned int nrbufs, curbuf, buffers;
    unsigned int readers;    //读进程编号
    unsigned int writers;    //写进程编号
    unsigned int files;    
    unsigned int waiting_writers;
    unsigned int r_counter;
    unsigned int w_counter;
    struct page *tmp_page;
    struct fasync_struct *fasync_readers;
    struct fasync_struct *fasync_writers;
    struct pipe_buffer *bufs;    //管道缓冲区
    struct user_struct *user;
};
</code></pre><p>管道是作为一组VFS对象来实现的，它们用特殊的文件系统pipefs来组织。在<code>pipe</code>系统调用发生时，首先会为pipefs分配一个索引节点对象，并且对其进行初始化；随后分别创建一个读文件对象，和一个写文件对象，并对其中的f_ops设置为定义好的操作表地址。随后分配一个dentry，用它把两个文件对象和索引节点对象连接在一起。随后这两个文件描述符就被返回给了用户态进程，于是乎两个进程就能够通过文件描述符来读写数据了。  </p>
<p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/pipes.gif?raw=true" alt="">  </p>
<h4 id="命名管道FIFO"><a href="#命名管道FIFO" class="headerlink" title="命名管道FIFO"></a>命名管道FIFO</h4><p>管道是十分方便的一种结构，不过它存在一个缺点，那就是不能让任意两个进程共享一个管道。命名管道和管道一样，都是借助于文件系统实现的，它遵循“先进先出”的原则，同样依赖于一个内存缓冲区。不过与管道不同的地方在于，管道的索引节点在pipefs文件系统中，而命名管道的节点在系统的目录树里面，所以所有的进程都可以访问命名管道，而且能够用读/写方式来打开。<br>命名管道首先需要通过<code>mknod</code>或<code>mkfifo</code>创建一个FIFO设备文件。一旦创建了FIFO文件，进程就可以用<code>open</code>、<code>write</code>、<code>read</code>等文件操作对FIFO进行操作。在进程使用系统调用<code>open</code>时，<code>init_special_inode()</code>会把FIFO相关的索引节点对象，进行初始化，并且把<code>i_fop</code>设置为定义好的表的地址，并执行<code>fifo_open()</code>。  </p>
<h4 id="IPC"><a href="#IPC" class="headerlink" title="IPC"></a>IPC</h4><p>IPC是（Interprocess Communication）的缩写。它包含信号量、消息队列、共享内存三种方式。IPC在linux中被作为一种<strong>资源</strong>。<code>semget()</code>、<code>msgget()</code>、<code>shmget()</code>都以一个“关键字”作为参数，获得相应的IPC标识符，并且让进程能够通过标识符对资源进行访问。那么在不同的进程中，就可以通过同一关键字来访问IPC。<br>每一类IPC都有一个<code>ipc_ids</code>数据结构进行管理，其数据结构如下：  </p>
<pre><code>struct ipc_ids {
    int in_use;                //已经分配的资源数
    unsigned short seq;        //下一个分配位置序号
    struct rw_semaphore rwsem;
    struct idr ipcs_idr;    //用基数树来保存，记录了所有IPC条目
    int next_id;
};
</code></pre><p><code>kern_ipc_perm</code>对应一个IPC资源。<code>ipc_addid()</code>能够把一个kern_ipc_perm指针，添加到对应<code>ipc_ids</code>的基数树当中去。其结构如下，<code>key</code>是唯一的标识符。在对应的资源结构题中，都包含有一个kern_ipc_perm指针，它也是管理具体资源的关键所在。  </p>
<pre><code>struct kern_ipc_perm {
    spinlock_t    lock;
    bool        deleted;
    int        id;    
    key_t        key;        //IPC关键字
    kuid_t        uid;
    kgid_t        gid;
    kuid_t        cuid;
    kgid_t        cgid;
    umode_t        mode;
    unsigned long    seq;    //位置使用序号
    void        *security;
} ____cacheline_aligned_in_smp;  
</code></pre><h4 id="IPC信号量"><a href="#IPC信号量" class="headerlink" title="IPC信号量"></a>IPC信号量</h4><p>IPC信号量和内核中的信号量有相似之处，但实际上要比内核信号量复杂：首先IPC信号量可以包含多个信号量值，也就是保护多个独立的数据结构；其次IPC信号量还提供了失效的安全机制，用来处理进程意外死亡的情况。当然，它主要还是作为一种共享资源的访问控制手段，或者用于进程同步，不能传递大量的数据。其定义为<code>sem_array</code>。</p>
<pre><code>struct sem_array {
    struct kern_ipc_perm    sem_perm;    //对应的kern_ipc_perm结构
    time_t            sem_ctime;            /* last change time */
    struct sem        *sem_base;            //第一个sem结构指针
    struct list_head    pending_alter;    //阻塞替换数组的请求队列
                                        /* that alter the array */
    struct list_head    pending_const;    //阻塞没有替换数组的请求队列
                                        /* that do not alter semvals */
    struct list_head    list_id;        //用来取消信号量操作
    int            sem_nsems;                //信号量的总数
    int            complex_count;            /* pending complex operations */
    unsigned int        use_global_lock;
}; 
</code></pre><p>其中，<code>struct sem</code>的结构也很简单，除了信号量的值和上次操作的进程pid之外，它还有一个阻塞队列（在最新版本的linux中，这个队列被分成了两个）</p>
<pre><code>struct sem {
    int    semval;            //信号量的值
    int    sempid;            //上次操作的pid
    spinlock_t    lock;    /* spinlock for fine-grained semtimedop */
    struct list_head    pending_alter;    /* pending operations */
                                        /* that alter the array */
    struct list_head    pending_const;    /* pending complex operations */
                                        /* that do not alter semvals */
    time_t    sem_otime;    /* candidate for sem_otime */
} ____cacheline_aligned_in_smp;
</code></pre><p>具体的组织方式如图所示。每个IPC信号量，都分配了一个挂起请求队列，它标识等待数组中信号量的进程。这也是一个FIFO队列，新的挂起请求都被追加到链表的末尾。<br>另一个值得注意的结构是<code>list_id</code>，它用来协助信号量的取消工作。它保存了某个进程对信号量所做的所有修改，如果说进程意外的退出了，那么就会把信号量的值恢复成正确的值。<br> <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E4%BF%A1%E5%8F%B7%E9%87%8F.png?raw=true =600x400" alt=""> </p>
<h4 id="IPC消息"><a href="#IPC消息" class="headerlink" title="IPC消息"></a>IPC消息</h4><p>IPC消息保存在一个IPC消息队列中，直到某个进程把它读走为止。它是内核中的消息链表，用队列的形式进行发送和接收，可以保存格式化的数据，并且缓冲区大，读写顺序完全可控。</p>
<pre><code>struct msg_queue {
    struct kern_ipc_perm q_perm;        //对应的kern_ipc_perm结构
    time_t q_stime;                        /* last msgsnd time */
    time_t q_rtime;                        /* last msgrcv time */
    time_t q_ctime;                        /* last change time */
    unsigned long q_cbytes;                //队列中的字节数
    unsigned long q_qnum;                //队列中的消息数
    unsigned long q_qbytes;                /* max number of bytes on queue */
    pid_t q_lspid;                        /* pid of last msgsnd */
    pid_t q_lrpid;                        /* last receive pid */

    struct list_head q_messages;        //消息链表
    struct list_head q_receivers;        //接收消息的进程链表
    struct list_head q_senders;            //发送消息的进程链表
};
</code></pre><p>对于q_messages来说，每条消息都用一个struct <code>msg_msg</code>来表示：  </p>
<pre><code>struct msg_msg {
    struct list_head m_list;
    long m_type;
    size_t m_ts;        /* message text size */
    struct msg_msgseg *next;
    void *security;
    /* the actual message follows immediately */
};
</code></pre><p>真正的消息部分紧跟在<code>msg_msg</code>的内存区域之后：  </p>
<pre><code>struct msgbuf {
    __kernel_long_t mtype;          /* type of message */
    char mtext[1];                  /* message text */
};
</code></pre><p>整个消息队列的工作机制如图所示。用户可以通过一个整数值来进行标识，这就允许进程有选择的从消息队列中获取消息。只要进程从消息队列中读消息，内核就会把读的消息删除。发送消息和接收消息分别使用<code>msgsnd</code>和<code>msgrcv</code>函数来完成。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97.png?raw=true" alt="=600x400" title="">
                </div>
                <div class="image-caption">=600x400</div>
            </figure>
<h4 id="IPC共享内存"><a href="#IPC共享内存" class="headerlink" title="IPC共享内存"></a>IPC共享内存</h4><p>共享内存是IPC里面，最快的一种方式。其本质就是一块物理内存同时映射到多个进程各自的进程空间当中。相应的，每个进程都要在自己的地址空间中，增加一个新的内存区，他映射与这个共享内存区相关的页框。共享内存区用<code>shmid_kernel</code>来表示：  </p>
<pre><code>struct shmid_kernel /* private to the kernel */
{    
    struct kern_ipc_perm    shm_perm;    //kern_ipc_perm数据结构
    struct file        *shm_file;            //共享段的特殊文件
    unsigned long        shm_nattch;
    unsigned long        shm_segsz;
    time_t            shm_atim;
    time_t            shm_dtim;
    time_t            shm_ctim;
    pid_t            shm_cprid;
    pid_t            shm_lprid;
    struct user_struct    *mlock_user;

    /* The task created the shm object.  NULL if the task is dead. */
    struct task_struct    *shm_creator;
    struct list_head    shm_clist;    /* list by creator */
};
</code></pre><p><code>shmid_kernel</code>中，一个很重要的域就是<code>shm_file</code>，因为共享段实际上是一个特殊的文件。我们知道在<code>vm_area_struct</code>当中，包含一个<code>vm_file</code>，也就是映射文件的域。不过shm文件系统并没有对应到目录树当中去，所以其操作只有mmap。对于共享内存映射来说，会通过<code>address_space</code>把页框包含在页高速缓存当中去。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98.png?raw=true =600x400" alt="">  </p>
<h4 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h4><p>Socket与其它方法的不同之处在于，它能够用于不同机器之间的进程通信。它与网络相关，涉及到具体的协议；作为本机通信时，可以设置对应的参数为<code>AF_UNIX</code>或<code>AF_INET</code>。注意，这里有两种方法，第一种是依然利用网络socket，把地址设置为localhost，这样通信依然会经过网络协议栈；第二种是利用<strong>Donmain Socket</strong>，不需要经过打包拆包、计算校验等过程，其本质是创建一个socket类型的文件。<br>在服务端：应用程序用系统调用<code>socket</code>创建一个套接字，并且用系统调用<code>bind</code>，将其绑定到IP地址和端口号上去，随后监听这个套接字，并且通过<code>accept</code>接收请求。在确定建立请求之后，通过<code>send</code>可以与客户端进行交互。    </p>
<pre><code>server_sockfd = socket(AF_INET, SOCK_STREAM, 0);  
server_addr.sin_family = AF_INET;                //指定网络套接字  
server_addr.sin_addr.s_addr = htonl(INADDR_ANY);//接受所有IP地址的连接  
server_addr.sin_port = htons(9736);                //绑定端口   
bind(server_sockfd, (struct sockaddr*)&amp;server_addr, sizeof(server_addr));//绑定套接字     
listen(server_sockfd, 5);监听套接字，建立一个队列

while(1){
    client_fd = accept(sockfd, (struct sockaddr*)&amp;remote_addr, &amp;sin_size));
    //创建子进程处理连接
    if(!fork()){
        if(send(client_fd, &quot;Hellp, you are connected!&quot;, 26, 0) == -1)
        ...
    }
}   
</code></pre><p>在客户端：同样用系统调用<code>socket</code>创建一个套接字，但是用<code>connect</code>函数来尝试建立连接。在连接之后，使用<code>recv</code>系统调用接收服务器的信息。  </p>
<pre><code>sockfd = socket(AF_INET, SOCK_STREAM,0)) == -1;

serv_addr.sin_family = AF_INET;  
serv_addr.sin_port = htons(SERVPORT);  
serv_addr.sin_addr = *((struct in_addr*)host-&gt;h_addr);  
bzero(&amp;(serv_addr.sin_zero), 8);  

//面向连接的socket通信要用connect在客户端首先连接  
if(connect(sockfd, (struct sockaddr *)&amp;serv_addr, sizeof(struct sockaddr)) == -1)  
{  
    perror(&quot;connect 出错！&quot;);  
    exit(1);  
}  

//用于接收服务器的反馈信息  
if((recvbytes = recv(sockfd, buf, MAXDATASIZE, 0)) == -1)  
{  
    perror(&quot;recv出错！&quot;);  
    exit(1);  
}  
</code></pre><h4 id="内存映射-amp-信号"><a href="#内存映射-amp-信号" class="headerlink" title="内存映射&amp;信号"></a>内存映射&amp;信号</h4><p>内存映射其实就是在两个进程中，同时映射一个文件，然后通过文件中的内容进行通信。<br>信号也可以用于进程通信。之前的博文已经详细说明了，不再赘述。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;管道&quot;&gt;&lt;a href=&quot;#管道&quot; class=&quot;headerlink&quot; title=&quot;管道&quot;&gt;&lt;/a&gt;管道&lt;/h4&gt;&lt;p&gt;管道是进程间的一个单向数据流；进程写入管道的数据都由内核定向到另一个进程，随后另一个进程由此从管道中读取数据，&lt;strong&gt;两个进程必须有
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解信号</title>
    <link href="http://sec-lbx.tk/2017/04/27/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E4%BF%A1%E5%8F%B7/"/>
    <id>http://sec-lbx.tk/2017/04/27/深入理解信号/</id>
    <published>2017-04-27T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="信号"><a href="#信号" class="headerlink" title="信号"></a>信号</h4><p>信号用来通知进程异步事件，可以把它理解为对中断的一种模拟。它是一个很小的消息，用来达到两个目的：（1）告知进程发生了一个特定的事件；（2）强迫进程执行自身所包含的信号处理程序。<br>linux预先定义了一些常规信号，并为它们定义了一些缺省操作。除此之外，还有一类实时信号，它们需要排队进行处理，我们也可以自己定义信号和信号处理方式。<br>既然信号是和进程相关的，那么<code>task_struct</code>中就必然包含有与信号相关的域了。</p>
<pre><code>task_struct{
    ...
    struct signal_struct *signal;        //进程信号描述符
    struct sighand_struct *sighand;        //进程信号处理程序描述符
    sigset_t blocked;                    //被阻塞信号掩码
    sigset_t real_bloced;                //被阻塞信号临时掩码
    struct sigpending pending;            //存放私有挂起信号
    ...
}
</code></pre><p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B/%E4%BF%A1%E5%8F%B7.gif?raw=true" alt="">  </p>
<h4 id="信号的产生"><a href="#信号的产生" class="headerlink" title="信号的产生"></a>信号的产生</h4><p>信号是由内核函数产生的，它们完成信号处理的第一步，也即更新一个/多个进程的描述符。产生的信号并不直接传递，而是根据信号的类型、目标进程的状态唤醒进程，让它们来接收信号。内核提供了一组产生信号的函数，包括为进程、线程组产生信号等，但它们最终都会调用<code>__send_signal()</code>。当然，在调用<code>__send_signal()</code>之前，会检查这个信号是否应该被忽略（进程没有被跟踪、信号被阻塞，显示忽略信号）   </p>
<pre><code>static int __send_signal(int sig, struct siginfo *info, struct task_struct *t,
        int group, int from_ancestor_ns)
{
struct sigpending *pending;
struct sigqueue *q;    
int override_rlimit;
int ret = 0, result;

assert_spin_locked(&amp;t-&gt;sighand-&gt;siglock);

result = TRACE_SIGNAL_IGNORED;
if (!prepare_signal(sig, t,
        from_ancestor_ns || (info == SEND_SIG_FORCED)))
    goto ret;
//获取进程或线程组的私有挂起队列
pending = group ? &amp;t-&gt;signal-&gt;shared_pending : &amp;t-&gt;pending;

//这个信号已经挂起了，忽略它
result = TRACE_SIGNAL_ALREADY_PENDING;
if (legacy_queue(pending, sig))
    goto ret;

result = TRACE_SIGNAL_DELIVERED;
//如果是kernel内部的某些强制信号，就立马执行
if (info == SEND_SIG_FORCED)
    goto out_set;

//如果没有超过挂起信号的上限
if (sig &lt; SIGRTMIN)
    override_rlimit = (is_si_special(info) || info-&gt;si_code &gt;= 0);
else
    override_rlimit = 0;

//产生一个sigqueue对象，并把它加入到队列中去
q = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE,
    override_rlimit);
if (q) {
    list_add_tail(&amp;q-&gt;list, &amp;pending-&gt;list);
    switch ((unsigned long) info) {
    case (unsigned long) SEND_SIG_NOINFO:
        q-&gt;info.si_signo = sig;
        q-&gt;info.si_errno = 0;
        q-&gt;info.si_code = SI_USER;
        q-&gt;info.si_pid = task_tgid_nr_ns(current,
                        task_active_pid_ns(t));
        q-&gt;info.si_uid = from_kuid_munged(current_user_ns(), current_uid());
        break;
    case (unsigned long) SEND_SIG_PRIV:
        q-&gt;info.si_signo = sig;
        q-&gt;info.si_errno = 0;
        q-&gt;info.si_code = SI_KERNEL;
        q-&gt;info.si_pid = 0;
        q-&gt;info.si_uid = 0;
        break;
    default:
        copy_siginfo(&amp;q-&gt;info, info);
        if (from_ancestor_ns)
            q-&gt;info.si_pid = 0;
        break;
    }

    //......
}
</code></pre><p>在信号产生之后，linux会调用<code>signal_wake_up()</code>通知进程，告知有新的挂起信号到来，如果当前进程占有了CPU，那么就可以立即执行；否则则要强制进行重新调度。</p>
<h4 id="信号的传递"><a href="#信号的传递" class="headerlink" title="信号的传递"></a>信号的传递</h4><p>在信号产生之后，如何确保挂起的信号被正确的处理呢？进程在信号产生时，可能并不在CPU上运行。在进程恢复用户态执行时，会进行检查，如果存在非阻塞的挂起信号，就调用<code>do_signal()</code>，这个函数会逐个助理挂起的非阻塞信号，而信号的处理则进一步调用<code>handle_signal()</code>。  </p>
<pre><code>handle_signal(struct ksignal *ksig, struct pt_regs *regs)
{
    bool stepping, failed;
    struct fpu *fpu = &amp;current-&gt;thread.fpu;

    //是否处于系统调用中
    if (syscall_get_nr(current, regs) &gt;= 0) {
        //系统调用被打断了，没有执行完，需要重新执行
        switch (syscall_get_error(current, regs)) {
        case -ERESTART_RESTARTBLOCK:
        case -ERESTARTNOHAND:
            regs-&gt;ax = -EINTR;
            break;

        case -ERESTARTSYS:
            if (!(ksig-&gt;ka.sa.sa_flags &amp; SA_RESTART)) {
                regs-&gt;ax = -EINTR;
                break;
            }
        /* fallthrough */
        case -ERESTARTNOINTR:
            regs-&gt;ax = regs-&gt;orig_ax;
            regs-&gt;ip -= 2;
            break;
        }
    }

    //设置栈帧
    failed = (setup_rt_frame(ksig, regs) &lt; 0);
    if (!failed) {
        regs-&gt;flags &amp;= ~(X86_EFLAGS_DF|X86_EFLAGS_RF|X86_EFLAGS_TF);
        /*
         * Ensure the signal handler starts with the new fpu state.
         */
        if (fpu-&gt;fpstate_active)
            fpu__clear(fpu);
    }
    signal_setup_done(failed, ksig, stepping);
}
</code></pre><p>这里存在一个问题：<code>handle_signal()</code>处于内核态中，但信号处理程序是在用户态定义的，因此这里存在着堆栈转换的问题。linux采用的方法是：把内核态堆栈中的硬件上下文，拷贝到当前进程的用户态堆栈中。而当信号处理程序完成时，会自动调用<code>sigreturn()</code>把硬件上下文拷贝回内核态堆栈中，并且恢复用户态堆栈中的内容。这里需要构造一个用户态栈帧：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B/%E6%A0%88.gif?raw=true" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>  
<p>首先内核需要把内核栈中的内容复制到用户态堆栈中去，把内核态堆栈的返回地址修改为信号处理程序的入口。注意，为了让信号处理程序结束时，能够清除栈上的内容，用户态堆栈还应该放入一个信号处理程序的返回地址，它指向<code>__kernel_sigreturn()</code>，把硬件上下文拷贝到内核态堆栈，然后把这个栈帧删除，随后从内核态返回到用户态继续执行。</p>
<h4 id="信号的接口"><a href="#信号的接口" class="headerlink" title="信号的接口"></a>信号的接口</h4><p><code>kill</code>/<code>tkill</code>/<code>kgill</code>系统调用分别用来给某个进程、线程组发送信号。其中，<code>kill(pid, sig)</code>分别接受一个进程的pid号，以及一个所发送的信号。<br>实时信号的发送则应该使用<code>rt_sigqueueinfo()</code>来进行发送。如果用户需要为信号指定一个操作，那么则应该使用<code>sigaction(sig, &amp;act, &amp;oact)</code>系统调用，<code>act</code>为指定的操作，而<code>old_act</code>用来记录以前的信号。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;信号&quot;&gt;&lt;a href=&quot;#信号&quot; class=&quot;headerlink&quot; title=&quot;信号&quot;&gt;&lt;/a&gt;信号&lt;/h4&gt;&lt;p&gt;信号用来通知进程异步事件，可以把它理解为对中断的一种模拟。它是一个很小的消息，用来达到两个目的：（1）告知进程发生了一个特定的事件；（2）强
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解进程地址空间</title>
    <link href="http://sec-lbx.tk/2017/04/25/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%BF%9B%E7%A8%8B%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4/"/>
    <id>http://sec-lbx.tk/2017/04/25/深入理解进程地址空间/</id>
    <published>2017-04-25T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="线性区"><a href="#线性区" class="headerlink" title="线性区"></a>线性区</h4><p>与内核中的内存分配不同，进程对内存的请求，被认为是不紧迫的。因此内核使用了一种新的资源，也就是<strong>线性区</strong>。应用程序申请动态内存时，并没有直接获得请求的页框，而是仅仅获得了一个新的<strong>线性区</strong>使用权。进程的地址空间，就是由所有的“线性区”来表示的，它的所有信息放在<code>mm_struct</code>当中（也就是我们在task_struct当中看到的），定义在mm_types.h当中。<br><code>mm_struct</code>包含了一个<code>vm_area_struct *mmap</code>，它也就是进程所拥有的所有线性区的链表。来看看vm_area_struct的结构：</p>
<pre><code>vm_area_struct{
    struct mm_struct *vm_mm;//指向线性区所在的mm_struct
    unsigned long vm_start;    //线性区的起始地址
    unsigned long vm_end;    //线性区的结束地址

    struct vm_area_struct *vm_next, *vm_prev;
    pgprot_t vm_page_prot;//线性区的访问权限
    unsigned long vm_flags;//标志位

    struct{
        struct rb_node rb;
        unsigned long rb_subtree_last;
    }shared;
    struct list_head anno_vma_chain;
    struct anon_vma *anon_vma;
    //以上均为链接到反映射所使用的数据结构

    const struct vm_operations_struct *vm_ops;
    //处理这个结构体的函数指针

    unsigned long vm_pgoff;
    struct file *vm_file;
    void *vm_private_data;
    //与映射文件、back store相关

}
</code></pre><p>线性区的组织方式如下。在内核中，查找包含制定线性地址的线性区是一个很频繁的操作，而进程的线性区可能有很多个，那么直接在链表上进行查找、插入会十分的麻烦。所以<code>mm_struct</code>当中有一个<code>rb_root</code>结构，它把所有的线性区组织了起来。<br>我们知道，内存的管理是由<code>page</code>作为一个最小单元的，那么页和线性区是什么关系呢？每个线性区都是一组连续的页构成的。<code>vm_flags</code>中存放的启示就是和页相关的信息：比如这个页的读写权限，是否可以共享，是否映射一个可执行文件等。<code>vm_page_prot</code>会被用来初始化一个新的页表项的值。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E7%BA%BF%E6%80%A7%E5%8C%BA.gif?raw=true" alt="">  </p>
<h4 id="线性区的查找、分配、释放"><a href="#线性区的查找、分配、释放" class="headerlink" title="线性区的查找、分配、释放"></a>线性区的查找、分配、释放</h4><p>假设给了一个虚拟地址addr，如何找到对应的vm_area_struct呢？这就需要在<code>rb_root</code>红黑树进行相应的搜索。<code>find_vma</code>函数就完成了这个工作，不过它会首先check之前最后访问的线性区，如果不在这个缓存中，再进行红黑树的查找。<code>get_unmapped_area</code>则会获取一个满足要求的空闲线性区。  </p>
<p>线性区的分配调用了<code>do_mmap</code>函数（在内核符号表中也能查到）。实际上我们所调用的<code>mmap</code>最后都会走向这个函数。</p>
<pre><code>unsigned long do_mmap(struct file *file, unsigned long addr,
        unsigned long len, unsigned long prot,
        unsigned long flags, vm_flags_t vm_flags,
        unsigned long pgoff, unsigned long *populate,
        struct list_head *uf)
</code></pre><p>如果指定一个<code>file</code>，则是把文件映射到内存中去，<code>offset</code>是偏移量；<code>addr</code>是希望从哪个地址开始查找一个空闲的区间。<code>len</code>则是线性区的长度。<code>vm_flags</code>指定了线性区的标志，而<code>flags</code>则是页的权限。具体的映射相关的工作由<code>mmap_region</code>函数来完成。  </p>
<p>（1）参数的检查和设置；<br>（2）获取新线性区的线性地址区间<code>get_unmmaped_area</code>；<br>（3）检查是否超过了地址空间的限制；<br>（4）如果有必要，会把旧的页映射关系给清除掉；<br>（5）如果可以，调用<code>vma_merge</code>直接把原来的vma进行拓展；<br>（6）调用<code>kmem_cache_alloc()</code>位新的线性区分配一个<code>vma_area_struct()</code>；<br>（7）调用<code>vma_link()</code>把新的线性区插入到线性区链表、红黑树中；  </p>
<p>而释放线性地址区间是由<code>do_munmap()</code>来完成的。它可能会涉及把一个线性区拆分为两个较小区的操作，也即<code>split_vma</code>所完成的工作。  </p>
<h4 id="缺页异常的处理"><a href="#缺页异常的处理" class="headerlink" title="缺页异常的处理"></a>缺页异常的处理</h4><p>缺页异既可能是由进程地址空间还没有分配物理页框引起的，又可能是由变成错误所引起的异常。所以linux的缺页异常处理，必须能够对各种情况进行处理，而我们前面所说的vm_area_struct正方便了这个处理的过程。<code>do_page_fault</code>是缺页中断的服务程序，它把引起缺页的线性地址和当前进程的线性地址比较，并选择适当方法去处理这个异常。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/pagefualt.gif?raw=true" alt="">  </p>
<p>缺页异常之所以复杂，是因为它涉及到内核态、用户态、中断等内容。<code>do_page_fault</code>接受两个参数：<code>pt__regs *regs</code>和<code>unsigned long error_code</code>。前者是异常发生时，寄存器的值；而error_code则说明了异常产生的状态：（1）访问了不存在的页（2）由读访问或执行访问引起（3）异常发生在内核态或用户态。这里，<code>do_page_fault</code>首先获取cr2寄存器的值，也即异常发生的地址，然后调用<code>__do_page_fault</code>。  </p>
<pre><code>static noinline void
__do_page_fault(struct pt_regs *regs, unsigned long error_code,
    unsigned long address)
{
struct vm_area_struct *vma;
struct task_struct *tsk;
struct mm_struct *mm;
int fault, major = 0;
unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;

tsk = current;
mm = tsk-&gt;mm;

//mmio不应该发生缺页
if (unlikely(kmmio_fault(regs, address)))
    return;

//缺页异常发生在内核态
if (unlikely(fault_in_kernel_space(address))) {
    if (!(error_code &amp; (PF_RSVD | PF_USER | PF_PROT))) {
        //缺页是否发生在vmalloc区
        //主要从内核页表向进程页表同步数据
        if (vmalloc_fault(address) &gt;= 0)
            return;

        if (kmemcheck_fault(regs, address, error_code))
            return;
    }

    //由陈旧TLB造成的异常，TLB没有flush，就flush TLB
    if (spurious_fault(error_code, address))
        return;

    return;
}

//这里是内核出现错误的情况
if (unlikely(smap_violation(error_code, regs))) {
    bad_area_nosemaphore(regs, error_code, address, NULL);
    return;
}

//异常处于用户态

//处于中断，没有用户上下文，属于错误情况
if (unlikely(faulthandler_disabled() || !mm)) {
    bad_area_nosemaphore(regs, error_code, address, NULL);
    return;
}

//开中断，因为cr2已经被保存了
if (user_mode(regs)) {
    local_irq_enable();
    error_code |= PF_USER;
    flags |= FAULT_FLAG_USER;
} else {
    if (regs-&gt;flags &amp; X86_EFLAGS_IF)
        local_irq_enable();
}

//读取产生异常的原因  
if (error_code &amp; PF_WRITE)
    flags |= FAULT_FLAG_WRITE;
if (error_code &amp; PF_INSTR)
    flags |= FAULT_FLAG_INSTRUCTION;


if (unlikely(!down_read_trylock(&amp;mm-&gt;mmap_sem))) {
    //异常发生在内核上下文，只能是异常表中预先定义好的异常
    if ((error_code &amp; PF_USER) == 0 &amp;&amp;
        !search_exception_tables(regs-&gt;ip)) {
        bad_area_nosemaphore(regs, error_code, address, NULL);
        return;
    }
retry:
    //如果在用户态、或者异常表中有对应的处理，说明不是内核异常
    down_read(&amp;mm-&gt;mmap_sem);
} else {
    might_sleep();
}

//在当前进程地址空间中，寻找发生异常的地址对应的VMA
vma = find_vma(mm, address);
//没有找到？说明是一个错误情况，要发出信号
if (unlikely(!vma)) {
    bad_area(regs, error_code, address);
    return;
}
//确认地址在有效的范围之内，是一个正常的缺页异常
if (likely(vma-&gt;vm_start &lt;= address))
    goto good_area;
//异常不是堆栈区紧挨的区且没有VMA
if (unlikely(!(vma-&gt;vm_flags &amp; VM_GROWSDOWN))) {
    bad_area(regs, error_code, address);
    return;
}

if (error_code &amp; PF_USER) {
    //超过了栈顶的范围
    if (unlikely(address + 65536 + 32 * sizeof(unsigned long) &lt; regs-&gt;sp)) {
        bad_area(regs, error_code, address);
        return;
    }
}

//需要拓展堆栈的情况
if (unlikely(expand_stack(vma, address))) 
    bad_area(regs, error_code, address);
    return;
}

//终于，是一个正常的缺页异常，要进行调页
good_area:
if (unlikely(access_error(error_code, vma))) {
    bad_area_access_error(regs, error_code, address, vma);
    return;
}

//分配物理内存，正常的处理函数
//1:请求调页
//2:COW
//3:页在交换分区
fault = handle_mm_fault(vma, address, flags);
major |= fault &amp; VM_FAULT_MAJOR;

//发送信号
up_read(&amp;mm-&gt;mmap_sem);
if (unlikely(fault &amp; VM_FAULT_ERROR)) {
    mm_fault_error(regs, error_code, address, vma, fault);
    return;
}

//兼容环境的检查
check_v8086_mode(regs, address, tsk);
}
</code></pre><p>可以看到，在处理正常的缺页异常之前，linux实际上已经做了很多检查了。<code>handle_mm_fault()</code>中，进一步调用了<code>__hanle_mm_fault()</code>。这个函数进行了一些页表的计算工作，然后把工作交给了<code>handle_pte_fualt</code>来处理。这是由于pte是最后一级页表项了，它的处理自然要特殊一些：  </p>
<pre><code>static int handle_pte_fault(struct vm_fault *vmf)
{
    pte_t entry;

    if (unlikely(pmd_none(*vmf-&gt;pmd))) {
        //暂时不填充pte，也许会申请大页
        vmf-&gt;pte = NULL;
    } else {
        if (pmd_devmap_trans_unstable(vmf-&gt;pmd))
            return 0;                

        //设置pte
        vmf-&gt;pte = pte_offset_map(vmf-&gt;pmd, vmf-&gt;address);
        vmf-&gt;orig_pte = *vmf-&gt;pte;

        barrier();
        if (pte_none(vmf-&gt;orig_pte)) {
            pte_unmap(vmf-&gt;pte);
            vmf-&gt;pte = NULL;
        }
    }

    //页表项不存在的情况
    //如果是非匿名页，那么就要把文件映射的内容读入映射页
    //如果是匿名页（堆栈），则分配全0的页
    if (!vmf-&gt;pte) {
        if (vma_is_anonymous(vmf-&gt;vma))
            return do_anonymous_page(vmf);
        else
            return do_fault(vmf);
    }

    //如果页不在内存中，但是页表项存在，说明这个页被换出了，现在应被换入
    if (!pte_present(vmf-&gt;orig_pte))
        return do_swap_page(vmf);

    vmf-&gt;ptl = pte_lockptr(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd);
    spin_lock(vmf-&gt;ptl);
    entry = vmf-&gt;orig_pte;
    if (unlikely(!pte_same(*vmf-&gt;pte, entry)))
        goto unlock;

    //写时复制的情况，这时要把页标识为脏页
    //如果有多个进程拥有这个页，那么写时复制就是有必要的
    //此时分配新的页框，并把内容复制到新的页框中去
    if (vmf-&gt;flags &amp; FAULT_FLAG_WRITE) {
        if (!pte_write(entry))
            return do_wp_page(vmf);
        entry = pte_mkdirty(entry);
    }

    entry = pte_mkyoung(entry);

    if (ptep_set_access_flags(vmf-&gt;vma, vmf-&gt;address, vmf-&gt;pte, entry,
                vmf-&gt;flags &amp; FAULT_FLAG_WRITE)) {
        update_mmu_cache(vmf-&gt;vma, vmf-&gt;address, vmf-&gt;pte);
    } else {
                    if (vmf-&gt;flags &amp; FAULT_FLAG_WRITE)
            flush_tlb_fix_spurious_fault(vmf-&gt;vma, vmf-&gt;address);
    }

unlock:
    pte_unmap_unlock(vmf-&gt;pte, vmf-&gt;ptl);
    return 0;
}
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;线性区&quot;&gt;&lt;a href=&quot;#线性区&quot; class=&quot;headerlink&quot; title=&quot;线性区&quot;&gt;&lt;/a&gt;线性区&lt;/h4&gt;&lt;p&gt;与内核中的内存分配不同，进程对内存的请求，被认为是不紧迫的。因此内核使用了一种新的资源，也就是&lt;strong&gt;线性区&lt;/strong
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解内存管理</title>
    <link href="http://sec-lbx.tk/2017/04/20/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    <id>http://sec-lbx.tk/2017/04/20/深入理解内存管理/</id>
    <published>2017-04-20T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="页框管理与伙伴系统"><a href="#页框管理与伙伴系统" class="headerlink" title="页框管理与伙伴系统"></a>页框管理与伙伴系统</h4><p>这里的内存管理，指的是内核如何分配（为自己）动态内存。linux把页框作为一个管理的基本单位，用数据结构<code>page</code>对其进行描述。而所有的<code>page</code>则放在一个<code>mem_map</code>数组当中，进行管理。但计算机存在着一些限制，因此linux把内存划分为了几个管理区，包括ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM等；而对页框的分配和释放，也是按照分区来进行管理的：<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E9%A1%B5%E6%A1%86%E5%88%86%E9%85%8D.jpg?raw=true" alt=""><br>在每个分区之内，页框由<strong>伙伴系统</strong>来进行处理。伙伴系统主要是为了解决“外碎片”的问题：当请求和释放不断发生的时候，就很有可能导致操作系统中发生存在空闲的小块页框，但是没有大块连续页框的问题。伙伴系统把空闲页分组成11个块链表，分别包含1，2，4，6,…,1024个连续的页框。每当有两个连续的大小为b的页框出现时（并且起始地址满足一个倍数条件），它们就被视为伙伴，伙伴系统就会把它们合并成大小为2b的页框。在页分配时，如果当前大小b的<code>free_list</code>中找不到空闲的页框，就会从2b的链表中寻找空闲页块，并且进行分割，将它分为两个大小为b的页块。<br>每个伙伴系统，管理的是<code>mem_map</code>的一个子集。在管理区描述符中，有一个<code>struct free_area</code>，它用来辅助伙伴系统：  </p>
<pre><code>struct free_area {
    struct list_head        free_list[MIGRATE_TYPES];
    unsigned long           nr_free;
}; 
</code></pre><p><code>free_list</code>是用来连接空闲页的链表数组，而nr_free则是当前内存区中空闲页块的个数。  </p>
<h4 id="反碎片"><a href="#反碎片" class="headerlink" title="反碎片"></a>反碎片</h4><p>当然，上面说到的只是最基本的伙伴系统，但它并没有完全解决碎片的问题。linux中还采用了一种反碎片的机制，它根据已内存页的类型来工作：<br>（1）不可移动页：在内存中有固定的位置，不能移动到其他地方（kernel的大多数内存页）<br>（2）可移动页：用户空间的页，只要更新页表项即可<br>（3）可回收页：在内存缺少时，可以进行回收的页，例如从文件映射的页<br>（以及一些其他类型）<br>如果根据页的可移动性，将其进行分组，避免可回收页和不可回收页的交叉组织（例如在可移动页中间有不可移动页），并且在某个类型的页分配失败时，会从备用列表中寻找相应的页，这个顺序定义在page_alloc.c当中。 </p>
<h4 id="内存分配方法"><a href="#内存分配方法" class="headerlink" title="内存分配方法"></a>内存分配方法</h4><p>分配内存通常可以调用一下几个函数：<br>alloc_pages/alloc_page：分配若干个页，返回第一个struct page<br>get_zeroed_page：分配一个struct page，并且将内存填0<br>get_free_pages/get_free_page：返回值是虚拟地址<br>get_dma_pages：分配一个适用于DMA的页<br>还有一些基于伙伴系统的方法，它们可能会借助页表进行映射，例如vmalloc，kmalloc。<br>内存分配时，通常要指定一个掩码<code>gfp_mask</code>，它定义了页所位于的区域、页在I/O和vfs上的操作，以及对分配操作的规定（阻塞、I/O、文件系统等）。</p>
<p>释放不再使用的页，同样可以采用struct page或者虚拟地址作为参数：<br>free_page/free_pages：以struct page为参数<br>__free_page/__free_pages：以虚拟地址为参数  </p>
<h4 id="页框高速缓存"><a href="#页框高速缓存" class="headerlink" title="页框高速缓存"></a>页框高速缓存</h4><p>（为了避免混淆，我把所有硬件的高速缓存称为cache）<br>内核经常会请求、释放单个页框，为了提高系统的性能，每个内存管理区都有一个每CPU的页框高速缓存，它包含一些预先分配的页框，能够用来满足CPU发出的单个页框请求。注意，这个页框高速缓存，和硬件上的cache的概念不同，但它们有一点小小的关联。由于每个CPU有自己的cache，那么假设一个进程刚刚释放了一个页，那么这个页就有很大概率还在cache当中。页框高速缓存保存热页（刚释放的，很可能在cache当中的页）和冷页（释放时间比较长的页）。其实对于分配热页来说，很好理解：用在cache中的页可以减少开销；但如果说是DMA设备使用，就要分配冷页了，因为它不会用到cache。  </p>
<h4 id="slab分配器"><a href="#slab分配器" class="headerlink" title="slab分配器"></a>slab分配器</h4><p>前面所说的伙伴系统，是用“页”为单位来进行，显然太大了；所以需要把页进一步拆分，变成更小的单位。slab分配器不仅仅提供小内存块，它还作为一个缓存使用，主要是针对那些经常分配、释放的对象：例如内核中的<code>fs_struct</code>数据结构，可能经常会分配和释放；那么slab就将释放的内存块保存在一个列表里面，而不是返回给伙伴系统。这样一来，再次分配新的内存块时，就不需要经过伙伴系统了，而且这些内存块还很可能在cache里面。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/slab%E5%88%86%E9%85%8D%E5%99%A8.png?raw=true =500x400" alt=""><br>slab分配器包含几个部分：高速缓存<code>kmem_cache</code>，slab，以及slab中所包含的对象。每个高速缓存只负责一种对象类型，它由多个<code>slab</code>构成。<code>kmem_cache</code>当中有三个slab链表，分别对应用尽的slab、部分空闲的slab，和空闲的slab，还有一个<code>array_cache *</code>数组，它保存cpu最后释放的那些很可能处于“热”状态的对象。<br>而对于每个slab，则组织了一系列的object；它包含了空闲对象，正在使用的对象。那么为什么不直接用<code>kmem_cache</code>管理对象，要增加出<code>slab</code>这一层呢？这明显是为了更好的管理内存：通过<code>slab</code>，可以让内存的使用更平均，或者能够更好的管理空闲的页。<br>在新版本的内核中，<code>slab</code>由<code>kmem_cache_node</code>来管理，它包含3个链表<code>slabs_partial</code>，<code>slabs_full</code>和<code>slabs_free</code>。每个slab是一个或多个连续页帧的集合，每个objects由链表串联，现在slab中的object直接由<code>page</code>中的<code>freelist</code>来管理了。  </p>
<pre><code>struct kmem_cache_node {
spinlock_t list_lock;

#ifdef CONFIG_SLAB
struct list_head slabs_partial;    /* partial list first, better asm code */
struct list_head slabs_full;
struct list_head slabs_free;
unsigned long free_objects;
unsigned int free_limit;
unsigned int colour_next;    /* Per-node cache coloring */
struct array_cache *shared;    /* shared per node */
struct alien_cache **alien;    /* on other nodes */
unsigned long next_reap;    /* updated without locking */
int free_touched;        /* updated without locking */
#endif

#ifdef CONFIG_SLUB
unsigned long nr_partial;
struct list_head partial;
#ifdef CONFIG_SLUB_DEBUG
atomic_long_t nr_slabs;
atomic_long_t total_objects;
struct list_head full;
#endif
#endif

};
</code></pre><p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/slab%E7%AE%A1%E7%90%86.png?raw=true =500x350" alt="">  </p>
<p>值得一提的是，<code>kmalloc</code>的实现也是也是基于slab来实现的，它包含一个数组，存放了一些用于不同长度的slab缓存，这也就是我们所说的“内存池”。  </p>
<h4 id="slab着色"><a href="#slab着色" class="headerlink" title="slab着色"></a>slab着色</h4><p>slab着色与颜色并没有关系，它要解决的问题与硬件高速缓存有关。硬件高速缓存倾向于把大小一样的对象放在高速缓存内的相同便宜位置；而不同slab当中相同偏移量的对象，就会映射在高速缓存的同一行当中；这样高速缓存可能就会频繁的对同一高速缓存行进行更新，从而造成性能损失。<br>slab着色就是给每个slab分配一个随机的“颜色”，把它作为slab中对象需要移动的特定偏移量来使用，这样对象就会被放置到不同的缓存行。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;页框管理与伙伴系统&quot;&gt;&lt;a href=&quot;#页框管理与伙伴系统&quot; class=&quot;headerlink&quot; title=&quot;页框管理与伙伴系统&quot;&gt;&lt;/a&gt;页框管理与伙伴系统&lt;/h4&gt;&lt;p&gt;这里的内存管理，指的是内核如何分配（为自己）动态内存。linux把页框作为一个管理的
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解进程与调度</title>
    <link href="http://sec-lbx.tk/2017/04/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%BF%9B%E7%A8%8B%E4%B8%8E%E8%B0%83%E5%BA%A6/"/>
    <id>http://sec-lbx.tk/2017/04/15/深入理解进程与调度/</id>
    <published>2017-04-15T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="进程、线程、轻量级进程"><a href="#进程、线程、轻量级进程" class="headerlink" title="进程、线程、轻量级进程"></a>进程、线程、轻量级进程</h4><p>进程是程序执行的一个实例，也是操作系统分配资源（CPU、内存）的对象，而线程是CPU执行和调度的最小单位。轻量级进程是linux中实现线程的方式，其本质也是一个进程，它与一个内核线程相关联，因此可以像普通程序一样被调度，但它只有最小的执行和调度信息（与普通程序比很“轻量”）。<br>在linux内核中，并没有线程的概念，每一个执行的实体，都是用linux的PCB：  <code>task_struct</code>来表示，它包含了进程的所有信息。进程的状态、内存、文件系统等信息，都放在这个结构体当中。  </p>
<h4 id="进程创建"><a href="#进程创建" class="headerlink" title="进程创建"></a>进程创建</h4><p>在linux当中，进程的创建都是通过子进程的方式来实现的。当然，并不是每个子进程都需要拷贝父进程的所有资源，因此linux也提供了三种不同的机制，来实现进程创建的问题：<br>（1）写时复制技术，允许父子进程读相同的物理页，只要两者中有一个试图写一个物理页时，就把这个页的内容拷贝到一个新的物理页中去，并分配给正在写的进程。<br>（2）轻量级进程允许父子进程共享内核中的页表、打开文件表等信息。<br>（3）vfork允许子进程共享副进程的内存地址空间，并通过阻塞父进程的方式防止数据被父进程修改。<br>linux中，通过这几个系统调用来完成进程的创建：<code>fork</code>、<code>vfork</code>、<code>clone</code>。那么为什么它们有什么区别呢？<br>sys_fork：创造的子进程是父进程的完整副本，复制所有的内容，运用了写时复制技术；<br>sys_vfork：创造的进程和副进程共享内存地址空间，并且子进程先于副进程运行；<br>sys_clone：创建线程，pthread库会间接地调用它；  </p>
<h4 id="进程调度"><a href="#进程调度" class="headerlink" title="进程调度"></a>进程调度</h4><p>进程调度，首先要对进程对状态有一个基本的了解。<code>task_struct</code>当中，<code>state</code>标识了一个进程的运行状态。  </p>
<p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B/%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81.jpg?raw=true" alt=""></p>
<p>linux中的进程调度，采用的是一种多级反馈队列的算法。首先，在linux中，进程可以被分为三类：（1）需要经常跟用户交互的<strong>交互式进程</strong>；（2）不必与用户交互，经常在后台运行的<strong>批处理进程</strong>；（3）有很强调度需要的实时进程。根据进程类型的不同，linux用多级队列的形式，去组织待调度的程序，并且动态地调整进程的优先级。进程的调度按照这样一个优先级进行：SCHED_FIFO，SCHED_RR，SHED_NORMAL。<br>那么进程的优先级是如何表示的呢？每个进程都有一个静态优先级，和一个动态优先级。静态优先级决定了进程的基本时间片；而动态优先级是调度新进程来运行时，所使用的数，它会根据进程的睡眠时间来进行改变。除此之外linux还用它来判断一个进程是交互式进程，还是批处理进程。<br>在linux当中，所有处于TASK_RUNNING状态（运行，或者就绪）的进程，会被放在一个<code>rq</code>（运行队列runqueue）中，它是一个每CPU变量。  </p>
<h4 id="调度过程"><a href="#调度过程" class="headerlink" title="调度过程"></a>调度过程</h4><p>进程的调度，是通过时钟来触发的。时钟会调用一个函数：<code>scheduler_tick()</code>。这个函数会通过调用，<code>curr-&gt;sched_class-&gt;task_tick</code>，也即根据当前进程的调度器类型，进行调度：如果是实时的进程，那么根据它调度的类型为SCHED_RR/SCHED_FIFO进行处理；如果是公平队列中的进程，就根据动态优先级和时间片的情况进行调度。<br><code>schedule()</code>函数完成具体的调度过程。这个函数可以在进程不能获得必需的资源时直接调用，也可以在进程用完时间片，或者被抢占时延迟调用。<br>如果说要唤醒一个睡眠或停止的进程，则会调用<code>try_to_wake_up()</code>函数，它把进程状态设置为TASK_RUNNING，并把进程插入<code>rq</code>。  </p>
<h4 id="进程切换"><a href="#进程切换" class="headerlink" title="进程切换"></a>进程切换</h4><p>在<code>schedule()</code>进行调度时，会发生进程的切换。该过程包括两部分：切换页全局目录和切换内核态堆栈与硬件上下文。<br>这里，我们先关注第二部分：在进程切换时，内核态的堆栈和硬件上下文发生了什么。这个工作由<code>switch_to()</code>宏来完成，它是一段汇编代码，并且接受3个参数：prev，next和last，它们都是<code>task_struct</code>。这里last是一个输出的变量，它用来保存切换之前的进程<code>task_struct</code>。这时因为在切换之后，<code>ebp</code>的值变了，所以prev和next都变成新的进程中的值了，这时就要把prev给修改掉，因此<code>schedule()</code>中调用的实际上是：  </p>
<pre><code>switch_to(prev, next, prev);
</code></pre><p>这里，把<code>next</code>中的栈地址装入<code>rsp</code>，就完成了切换。因为内核在next的内核栈上开始了操作。<strong>进程切换的一部分就是内核栈的切换</strong>；随后，内核跳转到<code>__switch_to()</code>函数继续执行。这个函数从完成保存和加载FPU、MMX、XMM、段寄存器（<strong>以及硬件上下文的切换</strong>），将TLS装入GDT表等一系列任务，最后返回schedule()中继续执行；在此之后进程还要修改<code>rq</code>的内容。  </p>
<h4 id="补充：僵死进程-守护进程"><a href="#补充：僵死进程-守护进程" class="headerlink" title="补充：僵死进程/守护进程"></a>补充：僵死进程/守护进程</h4><p>僵死进程：在进程退出时，它并没有真正的被销毁，其进程描述符还在内核中；必须由父进程调用<code>wait()</code>或者<code>waitpid()</code>系统调用，来为它收尸。如果说父进程没有做这件事情，那么进程描述符就会一直在内核中，他就是一个僵死进程。<br>守护进程：linux系统中常见的后台服务进程，它和任何终端无关，脱离了终端的控制；它也是一个常见的孤儿进程，其父进程是init。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;进程、线程、轻量级进程&quot;&gt;&lt;a href=&quot;#进程、线程、轻量级进程&quot; class=&quot;headerlink&quot; title=&quot;进程、线程、轻量级进程&quot;&gt;&lt;/a&gt;进程、线程、轻量级进程&lt;/h4&gt;&lt;p&gt;进程是程序执行的一个实例，也是操作系统分配资源（CPU、内存）的对象
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解I/O体系结构（二）</title>
    <link href="http://sec-lbx.tk/2017/04/11/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3I:O%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://sec-lbx.tk/2017/04/11/深入理解I:O体系结构（二）/</id>
    <published>2017-04-11T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="块设备的驱动"><a href="#块设备的驱动" class="headerlink" title="块设备的驱动"></a>块设备的驱动</h4><p>和字符设备类似，操作系统中的块设备，也是以文件的形式来访问。这里有一个很拗口的问题：磁盘是一个块设备，块设备有一个块设备文件。那么访问块设备文件和访问普通的磁盘上的文件有什么关系呢？<br>不论是块设备文件还是普通的文件，它们都是通过VFS来统一访问的。只不过对于一个普通文件，它可能已经在RAM中了（高速缓存机制），因此它的访问可能会直接在RAM中进行；但如果说要修改磁盘上的内容，或者文件内容不在RAM中，则也会间接地，通过块设备文件进行访问。这个驱动模型可以用这样一个图表示：<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%A3%81%E7%9B%98IO/%E5%9D%97%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8.png?raw=true" alt=""><br>这里我们只考虑最底层的情况：内核从块设备读取数据。为了从块设备中读取数据，内核必须知道数据的物理位置，而这正是<strong>映射层</strong>的工作。映射层的工作包括两步：（1）根据文件所在文件系统的块，将文件拆分成块，然后内核能够确定请求数据所在的块号；（2）映射层调用文件系统具体的函数，找到数据在磁盘上的位置，也就是完成文件块号，到逻辑块号的映射关系。<br>随后的工作在<strong>通用块层</strong>进行，内核在这一层，启动I/O操作。通常一个I/O操作对应一组连续的块，我们把它称为<code>bio</code>，它用来搜集底层需要的信息。<br><strong>I/O调度层</strong>负责根据内核中的各种策略，把待处理的I/O数据传送请求，进行归类。它的作用是把物理介质上相邻的数据请求，进行合并，一并处理。<br>最后一层也就是通过块设备的驱动来完成了，它向I/O接口发送适当的命令，从而进行实际的数据传送。</p>
<h4 id="通用块层"><a href="#通用块层" class="headerlink" title="通用块层"></a>通用块层</h4><p>通用块层负责处理所有块设备的请求，其核心数据结构就是<code>bio</code>。它代表<strong>一次块设备I/O请求</strong>。</p>
<pre><code>struct bio {
struct bio        *bi_next;        //请求队列中的下一个bio
struct block_device    *bi_bdev;    //块设备描述符指针
unsigned long        bi_flags;    /* status, command, etc */
unsigned long        bi_rw;        //rw位

struct bvec_iter    bi_iter;    

unsigned int        bi_phys_segments;//合并后有多少个段

unsigned int        bi_seg_front_size;
unsigned int        bi_seg_back_size;

atomic_t        bi_remaining;//剩余的bio_vec

bio_end_io_t        *bi_end_io;//bio结束的回调函数

void            *bi_private;

unsigned short        bi_vcnt;    //bio中biovec的数量

unsigned short        bi_max_vecs;//最多能有多少个

atomic_t        bi_cnt;        //结构体的使用计数

struct bio_vec        *bi_io_vec;    //bio_vec数组
};  
</code></pre><p>在这个数据结构中，还包含了一个<code>bio_vec</code>。这是什么意思呢？在linux中，相邻数据块被称为一个段，每个<code>bio_vec</code>对应一个内存页中的段。在io操作期间，bio是会一直更新的，其中的<code>bi_iter</code>用来在数组中遍历，按每个段来执行下一步的操作。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%A3%81%E7%9B%98IO/biovec.gif?raw=true" alt="">  </p>
<p>那么当通用块层收到一个I/O请求操作时，会发生什么呢？首先内核会为这次操作分配<code>bio</code>描述符，并对它进行填充。随后通用块层会调用<code>generic_make_request</code>，这个函数的作用很明确：它会进行一系列检查和设置，保证bio中的信息是<strong>针对整个磁盘，而不是磁盘分区的</strong>；随后获取这个块设备相关的请求队列q，调用<code>q-&gt;make_request_fn</code>，把bio插入请求队列中去。  </p>
<h4 id="I-O调度层"><a href="#I-O调度层" class="headerlink" title="I/O调度层"></a>I/O调度层</h4><p>在块设备上，每个I/O请求操作都是异步处理的，通用块层的请求会被加入块设备的请求队列中，每个块设备都会单独地进行I/O的调度，这样能够有效提高磁盘的性能。<br>前面提到，通用块层会调用一个<code>q-&gt;make_request_fn</code>，向I/O调度程序发送一个请求，该函数会进一步调用<code>__make_request()</code>。这个函数的目的，就是把<code>bio</code>放进请求队列当中：（1）如果请求队列是空的，就构造一个新的请求插入；（2）如果请求队列不是空的，但是<code>bio</code>不能合并（不能合并到某个请求的头和尾），也构造一个新的请求插入；（3）请求队列不是空的，并且<code>bio</code>可以合并，就合并到对应的请求中去。注意，bio，请求和请求队列的关系如下：  </p>
<pre><code>-- request_queue
        |-- request1
                |-- bio0
        |-- request2
                |-- bio1
                |-- bio2
</code></pre><p>而I/O的调度，就是对请求队列进行排序，针对磁盘的特点，降低寻道的次数。这里说说几个常见的算法：<br>（1）CFQ完全公平队列：默认的调度算法，完全公平排队。每个进程/线程都单独创建一个队列，并且用上面提到的策略进行管理。队列间采用时间片的方式来分配I/O。<br>（2）Deadline最后期限算法：在电梯调度的基础上，根据读写请求的“最后期限”进行排序，并通过读期限短于写期限来保证写操作不被饿死。<br>（3）预期I/O算法：与最后期限类似，但是在读操作时，会预先判断当前的进程是否马上会有读操作，并且优先地进行处理。<br>（4）NOOP：适用于固态硬盘，不进行任何优化。  </p>
<p> 总而言之，I/O调度层的作用，就是把请求的队列重新排序，并逐个交给块设备驱动程序进行处理。</p>
<h4 id="块设备驱动程序"><a href="#块设备驱动程序" class="headerlink" title="块设备驱动程序"></a>块设备驱动程序</h4><p>I/O调度层排序好的请求，会由块设备的驱动程序来处理。同样，块设备也遵循着我们前面提到的驱动程序模型：块设备对应一个<code>device</code>，而驱动程序对应了一个<code>device_driver</code>。对于块设备来说，驱动程序也要通过<code>register_blkdev()</code>注册一个设备号。随后，驱动程序要初始化<code>gendisk</code>描述符，以及它所包含的设备操作表<code>fops</code>。在此之后，是“请求队列”的初始化，以及中断程序的设置：要为设备注册IRQ线。最后要把磁盘注册到内核（<code>add_disk</code>）,并把它激活。<br>当一个块设备文件被<code>open()</code>时，内核同样也要为它初始化操作。对于块设备来说，其默认的文件操作如下：</p>
<pre><code>const struct file_operations def_blk_fops = {
.open        = blkdev_open,
.release    = blkdev_close,
.llseek        = block_llseek,
.read        = new_sync_read,
.write        = new_sync_write,
.read_iter    = blkdev_read_iter,
.write_iter    = blkdev_write_iter,
.mmap        = generic_file_mmap,
.fsync        = blkdev_fsync,
.unlocked_ioctl    = block_ioctl,
#ifdef CONFIG_COMPAT
.compat_ioctl    = compat_blkdev_ioctl,
#endif
.splice_read    = generic_file_splice_read,
.splice_write    = iter_file_splice_write,
};
</code></pre><p><code>dentry_open()</code>方法会调用<code>blkdev_open()</code>。它（1）首先会获取块设备的描述符：如果块设备已经打开，则可以通过inode-&gt;i_bdev直接获取，否则则需要根据设备号去查找块设备描述符。（2）获取块设备相关的<code>gendisk</code>地址，<code>get_gendisk</code>是通过设备号来找到gendisk的。（3）如果是第一次打开块设备，则要根据它是整盘还是分区，进行相应的设置和初始化。（4）如果不是第一次打开，只需要按需要执行自定义的<code>open()</code>函数就行了。  </p>
<h4 id="补充：I-O的监控方式"><a href="#补充：I-O的监控方式" class="headerlink" title="补充：I/O的监控方式"></a>补充：I/O的监控方式</h4><p>（1）轮询：CPU重复检查设备的状态寄存器，直到寄存器的值表明I/O操作已经完成了。<br>（2）中断：设备发出中断信号，告知I/O操作已经完成了，数据放在对应的端口，当数据缓冲满了时，由CPU去取，CPU需要控制数据传输的过程。<br>（3）DMA：由CPU的DMA电路来辅助数据的传输，CPU不需要参与内存和IO之间的传输过程，只需要通过DMA的中断来获取信息。DMA能够在所有数据处理完时才通知CPU处理。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;块设备的驱动&quot;&gt;&lt;a href=&quot;#块设备的驱动&quot; class=&quot;headerlink&quot; title=&quot;块设备的驱动&quot;&gt;&lt;/a&gt;块设备的驱动&lt;/h4&gt;&lt;p&gt;和字符设备类似，操作系统中的块设备，也是以文件的形式来访问。这里有一个很拗口的问题：磁盘是一个块设备，块设备
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解I/O体系结构（一）</title>
    <link href="http://sec-lbx.tk/2017/04/10/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3I:O%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://sec-lbx.tk/2017/04/10/深入理解I:O体系结构（一）/</id>
    <published>2017-04-10T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="I-O体系结构"><a href="#I-O体系结构" class="headerlink" title="I/O体系结构"></a>I/O体系结构</h4><p>虚拟文件系统利用底层函数，调用每个设备的操作，那么这些操作是如何在设备上执行的，操作系统又是如何知道设备的操作是什么的呢？这些是由操作系统决定的。<br>我们知道，操作系统的工作，是依赖于数据通路的，它们让信息得以在CPU、RAM、I/O设备之间传递。这些数据通路称为<strong>总线</strong>。这就包括数据总线（PCI、ISA、EISA、SCSI等）、地址总线、控制总线。I/O总线，指的就是用于CPU和I/O设备之间通信的<strong>数据总线</strong>。I/O体系的通用结构如图所示：  </p>
<p><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%A3%81%E7%9B%98IO/IO%E7%BB%93%E6%9E%84.png?raw=true =600x400" alt="">  </p>
<p>那么CPU是如何通过I/O总线和I/O设备交互呢？这首先得从内存和外设的编址方式说起。第一种是“独立编址”，也就是内存和外设分开编址，I/O端口有独立的地址空间，这也被称为<strong>I/O映射方式</strong>。每个连接到I/O总线上的设备，都分配了自己的I/O地址集（在I/O地址空间中），它被称为I/O端口。<code>in</code>、<code>out</code>等指令用语CPU对I/O端口进行读写。在执行其中一条指令时，CPU使用地址总线选择所请求的I/O端口，使用数据总线在CPU寄存器和端口之间传送数据。这种方式编码逻辑清晰，速度快，但空间有限。<br>第二种是“统一编址”，也被称为<strong>内存映射方式</strong>，I/O端口还可以被映射到内存地址空间（这也正是现代硬件设备倾向于使用的方式），这样CPU就可以通过对内存操作的指令，来访问I/O设备，并且和DMA结合起来。这种方式更加统一，易于使用。它实际上使用了<code>ioremap()</code>。<strong>自从PCI总线出现后，不论采用I/O映射还是内存映射方式，都需要将I/O端口映射到内存地址空间</strong>。  </p>
<p>每个I/O设备的I/O端口都是一组寄存器：控制寄存器、状态寄存器、输入寄存器和输出寄存器。内核会纪录分配给每个硬件设备的I/O端口。    </p>
<h4 id="设备驱动程序模型"><a href="#设备驱动程序模型" class="headerlink" title="设备驱动程序模型"></a>设备驱动程序模型</h4><p>在内核中，设备不仅仅需要完成相应的操作，还要对其电源管理、资源分配、生命周期等等行为进行统一的管理。因此，内核建立了一个统一的设备模型，提取设备操作的共同属性，进行抽象，并且为添加设备、安装驱动提供统一的接口。它们本身并不代表具体的对象，只是用来维持对象间的层次关系。<br>这里首先要提的是<strong>sysfs</strong>文件系统。和/proc类似，安装于/sys目录，其目的是表现出设备驱动程序模型间的层次关系。在驱动程序模型当中，有三种重要的数据结构（旧版本），自上到下分别是<code>subsystem</code>、<code>kset</code>、<code>kobject</code>。如果要理解这个模型中，每个数据结构的作用，就必须理解它们和操作系统中的什么东西相对应。它们均对应着<strong>/sys中的目录</strong>。<code>kobject</code>是这个对象模型中，所有对象的基类。<code>kset</code>本身首先是一个<code>kobject</code>，而它又承担着一个<code>kobject</code>容器的作用，它把<code>kobject</code>组织成有序的目录；subsys则是更高的一层抽象，它本身首先是一个<code>kset</code>。驱动、总线、设备都能够用设备驱动程序模型中的对象表示。   </p>
<h4 id="设备驱动程序模型中的组件"><a href="#设备驱动程序模型中的组件" class="headerlink" title="设备驱动程序模型中的组件"></a>设备驱动程序模型中的组件</h4><p>设备驱动程序模型建立在几个基本数据结构之上，它们描述了总线、设备、设备驱动器等等。这里，我们来看看它们的数据结构。首先，<code>device</code>用来描述设备驱动程序模型中的设备。  </p>
<pre><code>struct device {
struct device        *parent;//父设备
struct kobject kobj;        //对应的kobject
const char        *init_name; //初始化名

const struct device_type *type;//设备的类型

struct mutex        mutex;    //驱动的互斥量

struct bus_type    *bus;        //设备在什么类型的总线
struct device_driver *driver;    //设备的驱动

void        *driver_data;    //驱动私有数据指针
struct dev_pm_info    power;
struct dev_pm_domain    *pm_domain;
//dma相关变量
u64        *dma_mask;            
u64        coherent_dma_mask;    
unsigned long    dma_pfn_offset;
struct device_dma_parameters *dma_parms;
struct list_head    dma_pools;    
struct dma_coherent_mem    *dma_mem; 

dev_t            devt;    //dev目录下的描述符
u32            id;    

spinlock_t        devres_lock;
struct list_head    devres_head;

struct klist_node    knode_class;
struct class        *class;    //类

void    (*release)(struct device *dev);//释放设备描述符时候的回调函数
};
</code></pre><p>首先，可以看到<code>device</code>中包含有一个<code>kobject</code>，还包含有它相关驱动对象。所有的device对象，全部收集在devices_kset中，它对应着/sys/devices中。设备的引用计数则是由kobject来完成的。device还会被嵌入到一个更大的描述符中，例如<code>pci_dev</code>，它除了包含<code>dev</code>之外，还有PCI所特有的一些数据结构。<code>device_add</code>完成了新的device的添加工作。我注意到，<code>error = bus_add_device(dev);</code>，也就是说device的添加会把它和bus关联起来。  </p>
<hr>
<p>再来看看驱动程序的结构。其数据结构为<code>device_driver</code>。相对于设备的数据结构来说，它相对较为简单：对于每个设备驱动，都有几个通用的方法，分别用语处理热插拔、即插即用、电源管理、探查设备等。同样，驱动也会被嵌入到一个更大的描述符中，例如<code>pci_driver</code>。  </p>
<pre><code>struct device_driver {
const char        *name;        //驱动名
struct bus_type        *bus;    //总线描述符

struct module        *owner;
const char        *mod_name;    //模块名

bool suppress_bind_attrs;    /* disables bind/unbind via sysfs */

const struct of_device_id    *of_match_table;
const struct acpi_device_id    *acpi_match_table;

int (*probe) (struct device *dev);        //探测设备
int (*remove) (struct device *dev);    //移除设备
void (*shutdown) (struct device *dev);    //断电方法
int (*suspend) (struct device *dev, pm_message_t state);//低功率
int (*resume) (struct device *dev);    //恢复方法
const struct attribute_group **groups;

const struct dev_pm_ops *pm;    //电源管理的操作

struct driver_private *p;
};
</code></pre><p>为什么这里没有<code>kobject</code>呢？它实际上保存在了<code>driver_private</code>当中，这个结构和device_driver是双向链接的。  </p>
<pre><code>struct driver_private {
struct kobject kobj;
struct klist klist_devices;
struct klist_node knode_bus;
struct module_kobject *mkobj;
struct device_driver *driver;
};  
</code></pre><p>driver的添加，通过调用<code>driver_register()</code>来完成，它同样包含一个函数：<code>bus_add_driver()</code>，也就是将driver添加到某个bus。  </p>
<hr>
<p>再来看看总线的结构。bus是连接device和driver的桥梁，bus中的很多代码，都是为了让device找到driver来设计的。总线的数据结构如下：  </p>
<pre><code>struct bus_type {
const char        *name;
const char        *dev_name;
struct device        *dev_root;
struct device_attribute    *dev_attrs;    /* use dev_groups instead */
const struct attribute_group **bus_groups;
const struct attribute_group **dev_groups;
const struct attribute_group **drv_groups;
//检查驱动是否支持特定设备
int (*match)(struct device *dev, struct device_driver *drv);    //回调事件，在kobject状态改变时调用
int (*uevent)(struct device *dev, struct kobj_uevent_env *env);
//探测设备
int (*probe)(struct device *dev);
//从总线移除设备
int (*remove)(struct device *dev);
//掉电
void (*shutdown)(struct device *dev);    

int (*online)(struct device *dev);
int (*offline)(struct device *dev);

//改变电源状态和恢复
int (*suspend)(struct device *dev, pm_message_t state);
int (*resume)(struct device *dev);

const struct dev_pm_ops *pm;

const struct iommu_ops *iommu_ops;

struct subsys_private *p;
struct lock_class_key lock_key;
};
</code></pre><p>同样，总线也有一个<code>subsys_private</code>，它保存了kobject。<code>but_type</code>中定义了一系列的方法。例如，当内核检查一个给定的设备是否可以由给定的驱动程序处理时，就会执行<code>match</code>方法。可以用<code>bus_for_each_drv()</code>和<code>bus_for_each_dev()</code>函数分别循环扫描drivers和device两个链表中的所有元素，来进行match。   </p>
<h4 id="设备文件"><a href="#设备文件" class="headerlink" title="设备文件"></a>设备文件</h4><p>设备驱动程序使得硬件设备，能以特定方式，响应控制设备的编程接口（一组规范的VFS函数，open，read，lseek，ioctl等），这些函数都是由驱动程序来具体实现的。在设备文件上发出的系统调用，都会由内核转化为对应的设备驱动程序函数，因此设备驱动必须被注册，也即构造一个<code>device_driver</code>，并且加入到设备驱动程序模型中。在注册时，内核会试图进行一次match。注意，这个注册的过程基本<code>driver_register</code>通常不会在驱动中直接调用，但我们但驱动通常都会间接的调用它来完成注册。<br>遵循linux“一切皆文件”的原则，I/O设备同样可以当作设备文件来处理，它和磁盘上的普通文件的交互方式一样，例如都可以通过<code>write()</code>系统调用写入数据。设备文件可以通过<code>mknod()</code>节点来创建，它们保存在/dev/目录下。<br>linux当中，硬件设备可以花费为两种：字符设备和块设备。其中，块设备指的是可以随机访问的设备，例如硬盘、软盘等；而字符设备则指的是声卡、键盘这样的设备。设备文件同样在VFS当中，但它的索引节点没有指向磁盘数据的指针，相反地它对应一个标识符（包含一个主设备号和一个次设备号）。VFS会在设备文件打开时，改变一个设备文件的缺省文件操作，让它去调用和设备相关的操作。</p>
<h4 id="字符设备驱动程序"><a href="#字符设备驱动程序" class="headerlink" title="字符设备驱动程序"></a>字符设备驱动程序</h4><p>这里我们以字符设备驱动程序为例。首先，字符设备的驱动，在linux系统中，是以<code>cdev</code>结构来表示的：</p>
<pre><code>struct cdev {
struct kobject kobj;
struct module *owner;
const struct file_operations *ops;
struct list_head list;    //包括的inode的devices
dev_t dev;
unsigned int count;
};
</code></pre><p>现在让我们回顾一下inode的数据结构：</p>
<pre><code>struct inode {
    ...
    union {
    struct pipe_inode_info    *i_pipe;
    struct block_device    *i_bdev;
    struct cdev        *i_cdev;
};
    ...
}
</code></pre><p>我们看到了<code>cdev</code>指针的影子，可见cdev和inode确实是直接相关的。要实现驱动，首先就要对cdev进行初始化，注册字符设备。驱动的安装，首先要分配cdev结构体、申请设备号并初始化cdev。注意，驱动程序是如何和刚才我们所说的设备驱动模型建立联系的呢？实际上在初始化cdev的时候，就调用了<code>kobject_init()</code>，在模型中添加了一个<code>kobject</code>。<br>随后，驱动要注册cdev，也即调用<code>cdev_add()</code>函数。这个工作主要是由<code>kobj_map()</code>来实现的，它是一个数组。对于每一类设备，都有一个全局变量，例如字符设备的<code>cdev_map</code>，块设备的<code>bdev_map</code>。最后要进行硬件资源的初始化。  </p>
<pre><code>int cdev_add(struct cdev *p, dev_t dev, unsigned count)
{
    int error;

    p-&gt;dev = dev;
    p-&gt;count = count;

    error = kobj_map(cdev_map, dev, count, NULL,
             exact_match, exact_lock, p);
    if (error)
        return error;

    kobject_get(p-&gt;kobj.parent);

    return 0;
}  
</code></pre><p>kobj_map的结构如下，它用来保存设备号和kobject的对应关系</p>
<pre><code>struct kobj_map {
    struct probe {
        struct probe *next;
        dev_t dev;
        unsigned long range;
        struct module *owner;
        kobj_probe_t *get;
        int (*lock)(dev_t, void *);
        void *data;
    } *probes[255];
    struct mutex *lock;
};
</code></pre><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%A3%81%E7%9B%98IO/kobjmap.jpg?raw=true" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>  
<p>不过到现在为止，我们都还没有说明，程序在访问字符设备时，是如何去调用正确的方法的。我们曾提到过，<code>open()</code>系统调用会改变字符文件对象的f_op字段，将默认文件操作替换为驱动的操作。在字符设备文件创建时，会调用<code>init_special_inode</code>来进行索引节点对象的初始化。其inode的操作(def_chr_fops)只包含一个默认的文件打开操作，也即<code>chrdev_open</code>。它会根据inode，首先利用<code>cdev_map</code>，找到对应的kobject，随后再进一步找到cdev，然后从中提取出文件操作的函数<code>fops</code>，并把它填充到file当中去。    </p>
<pre><code>static int chrdev_open(struct inode *inode, struct file *filp)
{
    const struct file_operations *fops;
    struct cdev *p;
    struct cdev *new = NULL;
    int ret = 0;

    spin_lock(&amp;cdev_lock);
    p = inode-&gt;i_cdev;
    if (!p) {
        struct kobject *kobj;
        int idx;
        spin_unlock(&amp;cdev_lock);
        kobj = kobj_lookup(cdev_map, inode-&gt;i_rdev, &amp;idx);//获取对应的kobject
        if (!kobj)
            return -ENXIO;
        new = container_of(kobj, struct cdev, kobj);
        spin_lock(&amp;cdev_lock);
        /* Check i_cdev again in case somebody beat us to it while
           we dropped the lock. */
        p = inode-&gt;i_cdev;
        if (!p) {
            inode-&gt;i_cdev = p = new;
            list_add(&amp;inode-&gt;i_devices, &amp;p-&gt;list);//将device加入到cdev的list中去
            new = NULL;
        } else if (!cdev_get(p))
            ret = -ENXIO;
    } else if (!cdev_get(p))
        ret = -ENXIO;
    spin_unlock(&amp;cdev_lock);
    cdev_put(new);
    if (ret)
        return ret;

    ret = -ENXIO;
    fops = fops_get(p-&gt;ops)
    if (!fops)
        goto out_cdev_put;

    replace_fops(filp, fops);//替换file当中的fops      
    return ret;
}
</code></pre><p>这里很奇怪的是，我们并没有看到类似前面提到的<code>driver_register()</code>、<code>device_register()</code>这样的函数。实际上这里并没有真正创建一个设备，而只是说创建了一个接口，所以有这样一个这个问题：<a href="http://blog.csdn.net/luckywang1103/article/details/47860805" target="_blank" rel="external">为什么cdev_add没有产生设备节点？</a>对于这个问题，我们应该理解为<code>cdev</code>和<code>driver/device</code>二者是配套工作的，cdev用来和用户交互，而device则是内核中的结构。<br>另一个问题是，在上面的过程中，似乎没有提及设备文件的创建。实际上，作为一个rookie，那么设备文件常常是用<code>mknod</code>命令手动创建的。当然，linux自然也提供了自动创建的借口，那就是利用udev来实现，调用<code>device_create()</code>函数。<br>当然，这个例子只是为了说明，操作系统的驱动程序是如何工作的，为什么对I/O设备的操作可以抽象成对设备文件的操作，程序在操作I/O文件时，是如何使用正确的操作的。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;I-O体系结构&quot;&gt;&lt;a href=&quot;#I-O体系结构&quot; class=&quot;headerlink&quot; title=&quot;I/O体系结构&quot;&gt;&lt;/a&gt;I/O体系结构&lt;/h4&gt;&lt;p&gt;虚拟文件系统利用底层函数，调用每个设备的操作，那么这些操作是如何在设备上执行的，操作系统又是如何知道
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>IDAPython:进阶（一）</title>
    <link href="http://sec-lbx.tk/2017/04/05/IDAPython-advance1/"/>
    <id>http://sec-lbx.tk/2017/04/05/IDAPython-advance1/</id>
    <published>2017-04-05T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="IDAPython-进阶（一）"><a href="#IDAPython-进阶（一）" class="headerlink" title="IDAPython:进阶（一）"></a>IDAPython:进阶（一）</h4><h4 id="Instruction-feature"><a href="#Instruction-feature" class="headerlink" title="Instruction feature"></a>Instruction feature</h4><p>在IDAPython中，时常可以看到使用<code>insn.get_canon_feature()</code>来获取指令“特性”。这里，<code>instruc_t</code>类中包含有两个变量，<code>name</code>和<code>feature</code>，其中<code>feature</code>是一系列的指令特征位。 </p>
<h4 id="IDA-SDK"><a href="#IDA-SDK" class="headerlink" title="IDA SDK"></a>IDA SDK</h4><p>IDA SDK分为很多hpp，不过它的文档不是特别友好，这里我把比较重要的header的作用给列出来，方便日后进行分析和使用：  </p>
<ul>
<li>area:程序中地址范围的集合，它表示一段连续的地址范围，由起始地址和终止地址来表示，例如程序中的segments就是用area来表示的，它在IDA的数据库当中，是采用B树的形式来保存的。  </li>
<li>bitrange:用来管理一段连续bits的容器，类似一个数组。  </li>
<li>bytes:处理byte特性的函数，在程序中，每个byte都被识别成一个32-bit的值，他被IDA称为<code>flags</code>。对于bits和flags，都只能通过特定的函数去修改、访问。flags被保存在*.id1这样的文件当中。  </li>
<li>diskio:IDA的文件IO函数，通常不使用标准的C来进行I/O，而使用这个hpp当中的函数。  </li>
<li>entry:处理entry入口的函数，每个entry包含有地址、名称、序号。  </li>
<li>fixup:处理地址、偏移量的修正等。  </li>
<li>frame:处理栈桢，包括参数、返回值、保存的寄存器和局部变量等。  </li>
<li>funcs:处理反汇编程序的函数，函数由多个chunk组成。  </li>
<li>idp:包含IDP模块的接口，包括有目标的汇编器，以及当前处理器的信息。例如判断一条指令是否为jmp、ret等，都可以使用这种方式。</li>
<li>lines:处理反汇编text line的生成。</li>
<li>name:处理命名，给一个特定地址命名等，但是指令和数据的中间是不能命名的。  </li>
<li>netnode:database的底层接口，程序被以B树的形式保存，而B树的信息则是存放在netnode当中的。  </li>
<li>offset:用来处理offset的函数，一个操作数可能自身，或者一部分表示了程序中的偏移量。  </li>
<li>search:中间层的搜索函数，包括寻找数据、代码等。  </li>
<li>segments:用来和程序中段进行交互的函数，IDA需要程序中的所有地址，属于一个segment(每个地址都必须属于一个segment)，如果地址不属于一个segment，那么这些bytes不能被转换为指令、不能用有名字、不能拥有注释。每个segment都有一个selector。  </li>
<li>ua:处理程序指令的反汇编。它包含两种函数，第一类是通过kernel来反汇编，第二类是通过IDP模块来反汇编，它们是“helper”。反汇编可以分为三步：分析、仿真、转化为文字。  </li>
<li>xref:处理交叉引用的情况。包括有CODE和DATA的引用。  </li>
</ul>
<h4 id="获取文件名"><a href="#获取文件名" class="headerlink" title="获取文件名"></a>获取文件名</h4><p>SDK提供了两个api，idaapi.get_root_filename能够用来获取当前的文件名，而idaapi.get_input_file_path能够用来获取包含路径的文件名。这两个函数定义在nalt当中。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;IDAPython-进阶（一）&quot;&gt;&lt;a href=&quot;#IDAPython-进阶（一）&quot; class=&quot;headerlink&quot; title=&quot;IDAPython:进阶（一）&quot;&gt;&lt;/a&gt;IDAPython:进阶（一）&lt;/h4&gt;&lt;h4 id=&quot;Instruction-f
    
    </summary>
    
    
      <category term="系统安全" scheme="http://sec-lbx.tk/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"/>
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>IDAPython：入门</title>
    <link href="http://sec-lbx.tk/2017/04/05/IDAPython/"/>
    <id>http://sec-lbx.tk/2017/04/05/IDAPython/</id>
    <published>2017-04-05T13:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="取地址"><a href="#取地址" class="headerlink" title="取地址"></a>取地址</h4><p><code>ScreenEA()</code>和<code>here()</code>是最常用的取地址函数，它们会返回一个整数值。<br><code>MaxEA()</code>和<code>MinxEA()</code>可以用来取最大和最小地址。<br><code>GetDisasm()</code> <code>GetMnem()</code> <code>GetOpand()</code> 可以用来取某个地址中的指令、操作符、操作数等。  </p>
<h4 id="遍历：Segments，Functions-and-Instructions"><a href="#遍历：Segments，Functions-and-Instructions" class="headerlink" title="遍历：Segments，Functions,and Instructions"></a>遍历：Segments，Functions,and Instructions</h4><p>IDAPython的强大之处在于迭代。从<code>Segments()</code>开始是一个好的选择。这段代码能够遍历所有的segments。</p>
<pre><code>for seg in idautils.Segments():
    print idc.SegName(seg),    idc.SegStart(seg),    idc.SegEnd(seg)
</code></pre><p>同理的，对于Functions的遍历，可以有：</p>
<pre><code>for func in idautils.Functions():
    print hex(func), idc.GetFunctionName(func)
</code></pre><p>这里，<code>Functions()</code>是可以添加范围的，例如<code>Functions(start_addr, end_addr)</code>，而<code>get_func()</code>返回的函数，则还具有<code>startEA</code>和<code>endEA</code>属性，也即起起始和结束地址。这里，<code>get_func()</code>实际上是返回了一个类。<code>dir(class)</code>能够返回这个类中的成员。<br><code>idc.NextFunction(ea)</code>和<code>idc.PrevFunction(ea)</code>能够用来访问毗邻的两个函数。这里，<code>ea</code>只要是某个函数边界中的任何一个地址就可以了。利用这种方式来遍历也存在问题，如果不是函数的话，那么代码块会被IDA跳过。那么如果想在函数中遍历指令怎么办呢？<code>idc.NextHead</code>会帮你找到下一条指令。但这依赖于函数的边界，而且可能被jump影响。更好的方式是使用<code>idautils.FuncItems(ea)</code>，来遍历function当中的地址。<br>一旦知道了一个function，就可以遍历其中的指令。idautils.FuncItems(ea)返回的是一个指向<code>list</code>的迭代器。这个<code>list</code>包含了每一条指令的起始地址。以下是一个综合的应用，它输出一段代码中的非直接跳转。    </p>
<pre><code>for func in idautils.Functions():
    flags = idc.GetFunctionFlags(func)
    if flags &amp; FUNC_LIB or flags &amp; FUNC_THUNK:
        continue
    dism_addr = list(idautils.FuncItems(func)) 
    for line in dism_addr:
        m = idc.GetMnem(line)
        if m == &apos;call&apos; or m == &apos;jmp&apos;:
            op = idc.GetOpType(line, 0) 
            if op == o_reg:
                print &quot;0x%x %s&quot; % (line, idc.GetDisasm(line))
</code></pre><p>这里，<code>GetOpType</code>会返回一个整型值，它能被用来查看一个操作数是否为寄存器，内存引用。  </p>
<h4 id="Operands"><a href="#Operands" class="headerlink" title="Operands"></a>Operands</h4><p>前面也有提到，<code>GetOpType</code>能够用来获取操作数的类型。一共有这些类型：<br><code>o_void</code>表示不包含操作数<br><code>o_reg</code>表示一个通用寄存器<br><code>o_mem</code>表示直接内存引用<br><code>o_phrase</code>操作数包含基址寄存器+偏移寄存器<br><code>o_displ</code>操作数是由一个寄存器和一个偏移量组成的<br><code>o_imm</code>操作数直接是一个整型数<br><code>o_far</code>和<code>o_near</code>在x86和x86_64下均不常见。指用立即数表示的far/near地址<br><code>idaapi.decode_insn()</code>可用来对某个地址的指令进行解码。而<code>idaapi.cmd</code>则能用来提取指令中的各部分结构。  </p>
<h4 id="Searching"><a href="#Searching" class="headerlink" title="Searching"></a>Searching</h4><p>前文已经提到了一些搜索的方式，但有些时候可能会需要搜索一些特定的bytes序列，比如<code>0x55 0x8b 0xec</code>，这时就需要利用到<code>FindBinary</code>，对对应的格式进行搜索。其具体形式为<code>FindBinary(ea, flag, searchstr, radix)</code>。这其中，searchstr也即所搜索的pattern。radix是和CPU相关的，可以不写。  </p>
<pre><code>addr = MinEA()
for x in range(0,5):
    addr = idc.FindBinary(addr, SEARCH_DOWN|SEARCH_NEXT, pattern);
    if addr != idc.BADADDR:
        print hex(addr), idc.Getdisasm(addr)
</code></pre><p>与之类似的，还有<code>FindText(ea, flag, y, x, searchstr)</code>，这个函数可以用来搜索一些字符串，例如一些变量中的内容可能是一些特定的字符串，就可以利用这个函数进行搜索。此外，还有<code>isCode()</code>和<code>isData()</code>，<code>isTail()</code>，<code>isUnknown()</code>等函数，来判断一个地址的属性。<code>FindCode()</code>能够找到下一个被标记为代码的地址；而<code>FindData()</code>则能够返回下一个被标记为数据的地址。与之类似的<code>FindUnexplored()</code>，<code>FindExplored()</code>则是搜索下一个未识别／识别的地址。<code>FindImmediate()</code>则是找到特定的立即数。<br>不过，并不是每一次都需要对data/code进行搜索的。有时候，我们已经知道了code或者data的位置了，只是需要选择对应的内容进行分析。这时就可以用<code>SelStart()``SelEnd()</code>等一系列的函数。<br>对于数据、代码，还可以对其原始格式进行访问，也即它们的二进制形式。这些数据可以用<code>Byte()``Word()``Dword()``Qword()``GetFloat()``GetDouble()</code>来获取。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;取地址&quot;&gt;&lt;a href=&quot;#取地址&quot; class=&quot;headerlink&quot; title=&quot;取地址&quot;&gt;&lt;/a&gt;取地址&lt;/h4&gt;&lt;p&gt;&lt;code&gt;ScreenEA()&lt;/code&gt;和&lt;code&gt;here()&lt;/code&gt;是最常用的取地址函数，它们会返回一个整数值。&lt;
    
    </summary>
    
    
      <category term="IDA pro" scheme="http://sec-lbx.tk/tags/IDA-pro/"/>
    
      <category term="python" scheme="http://sec-lbx.tk/tags/python/"/>
    
      <category term="静态分析" scheme="http://sec-lbx.tk/tags/%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>深入理解文件系统</title>
    <link href="http://sec-lbx.tk/2017/03/30/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    <id>http://sec-lbx.tk/2017/03/30/深入理解文件系统/</id>
    <published>2017-03-30T03:40:00.000Z</published>
    <updated>2017-07-28T09:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="虚拟文件系统（VFS）的思想和作用"><a href="#虚拟文件系统（VFS）的思想和作用" class="headerlink" title="虚拟文件系统（VFS）的思想和作用"></a>虚拟文件系统（VFS）的思想和作用</h4><p>实际上，看到<strong>Virtaul</strong>，我们就应该想到VFS的作用，是虚拟出一个中间层，来实现某些部分的<strong>统一处理</strong>。实际上，VFS要解决的是，让应用能够“以一种通用的方式”，去访问不同类型的文件系统：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/VFS.png?raw=true" alt="=100" title="">
                </div>
                <div class="image-caption">=100</div>
            </figure><br>可以看到，对于不同类型的文件系统、网络文件系统、伪文件系统，应用对它们的操作接口都是一致的,而底层的驱动则会完成实际的区分工作。  </p>
<h4 id="通用文件模型"><a href="#通用文件模型" class="headerlink" title="通用文件模型"></a>通用文件模型</h4><p>VFS通过一个通用文件模型，来表示一个文件系统，它从顶层到底层，一共维护四个数据结构。这里我们通过磁盘文件系统来举例子说明：<br>file：文件<strong>对象</strong>。如果一个进程要和一个对象进行交互，那么久要在访问期间存放一个文件对象在内核内存中。<br>dentry：目录项<strong>对象</strong>。用来存放目录项和对应文件链接信息，每个磁盘文件系统，都有特殊的方式，将该类信息存在磁盘上。<br>inode：索引节点<strong>对象</strong>。用来存放关于具体文件的一般信息，对基于磁盘的文件系统来说，通常对应于存放在磁盘上的文件控制块，其节点号唯一标识文件系统中的文件。<br>superblock：超级块<strong>对象</strong>。对应安装的文件系统的信息，他对应磁盘上的文件系统控制块。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B.png?raw=true" alt="＝100" title="">
                </div>
                <div class="image-caption">＝100</div>
            </figure>
<p>这张图反映出了在一次访问中，各个对象是如何参与到过程中去的。这里我为什么要把<strong>对象</strong>给加粗呢？VFS实际采用的是一种面向对象的方式（虽然它是用C语言写的）。每个object，都有一系列的“操作方式”，VFS为这些对象提供了“统一的”接口，而具体的文件系统则提供了千差万别的实现方式，这是由一个函数指针表来实现的。  </p>
<h4 id="文件对象"><a href="#文件对象" class="headerlink" title="文件对象"></a>文件对象</h4><p>这里我们从文件对象开始说。如果留意进程的<code>task_struct</code>（定义在sched.h当中），你会发现一个<code>fs_struct</code>结构，以及一个<code>files_struct</code>结构。<br>其中<code>fs_struct</code>保存了当前点工作目录和根目录：</p>
<pre><code>struct fs_struct {
int users;
spinlock_t lock;
seqcount_t seq;
int umask;
int in_exec;
struct path root, pwd;
};
</code></pre><p>而<code>files_struct</code>结构则保存了当前进程所打开的所有文件，它们保存在<code>fdtable</code>中：</p>
<pre><code>struct fdtable {
unsigned int max_fds;
struct file __rcu **fd;      /* current fd array */
unsigned long *close_on_exec;
unsigned long *open_fds;
struct rcu_head rcu;
};
</code></pre><p>当<code>open()</code>系统调用发生时，文件描述符实际上是fd中的下标。现在我们终于看到<code>file</code>结构了，这里我只列出了重要的部分：</p>
<pre><code>struct file {
union {
    struct llist_node    fu_llist;
    struct rcu_head     fu_rcuhead;
} f_u;
struct path        f_path;
struct inode        *f_inode;    /* cached value */
const struct file_operations    *f_op;  

struct mutex        f_pos_lock;
loff_t            f_pos;

} __attribute__((aligned(4)));    
</code></pre><p>这里，<code>loff_t</code>是平时我们用来在文件读写上用于定位的指针，显然这个值必须放在file当中，因为可能会有几个进程同时访问一个文件。<br>而<code>f_op</code>正反映出了“通用文件模型的”特点，它保存了文件操作的对应指针；这个值是在进程打开文件时，从文件索引节点中的i_fop中复制的。<br>当然在<code>file</code>中我们也看到了<code>dentry</code>和<code>inode</code>的影子：<code>f_path</code>包含了<code>dentry *</code>，而*f_indoe也是file的一个域，它们分别指向了对应文件点dentry对象和inode对象，我们将会在接下来对它们进行详细的说明。  </p>
<h4 id="目录项对象"><a href="#目录项对象" class="headerlink" title="目录项对象"></a>目录项对象</h4><p>在上一节中，我们知道了文件对象描述了应用和文件的联系。这里，目录项是用来描述具体的文件系统中的目录项的，他的作用是：<strong>用来快速找到一个路径，并且与文件相关联</strong>。假设进程需要查找一个路径，那么路径中的每一个分量，都会有一个目录项与之对应。并且，目录项会把每个分量和它对应的索引节点联系起来。<code>dentry</code>的定义如下：  </p>
<pre><code>struct dentry {
unsigned int d_flags;        
seqcount_t d_seq;        
struct hlist_bl_node d_hash;    /* 哈希链表 */
struct dentry *d_parent;    /* 父目录项 */
struct qstr d_name;            /* 目录名 */
struct inode *d_inode;        /* 对应的索引节点 */
unsigned char d_iname[DNAME_INLINE_LEN];    /* small names */

struct lockref d_lockref;    /* per-dentry lock and refcount */
const struct dentry_operations *d_op;    /* dentry操作 */
struct super_block *d_sb;    /* 文件的超级块对象 */
unsigned long d_time;        
void *d_fsdata;            

struct list_head d_lru;        /* LRU list */
struct list_head d_child;    /* child of parent list */
struct list_head d_subdirs;    /* our children */

union {
    struct hlist_node d_alias;    /* inode alias list */
     struct rcu_head d_rcu;
} d_u;
};  
</code></pre><p>这里可以看到，除开自身的名称、引用计数等，目录项对象中有这些关键的结构：<code>d_op</code>描述了目录项所对应的操作（通用模型的特点）；<code>d_sb</code>则是    文件的超级块对象，至于<code>d_inode</code>则是这个目录项关联的索引节点（当然它们并不是一一对应的关系）。注意，这里还有个很重要的变量：<code>d_lockref</code>，它其实就是一个计数器，说明了这个目录项对象的引用次数。</p>
<p>目录项对象保存在<strong>dentry cache</strong>中。linux操作系统为了提高目录项对象的处理效率，设计了这个高速缓存。这是因为，从磁盘中读取目录项，并且构造相应的目录项对象是需要花费大量的时间的，因此，在完成对目录项对象的操作之后，在内存中（尽量）保存它们具有很重要的意义。这个dentry cache，其本质是一个哈希链表，它定义在list_bl.h当中（它的hash计算在<code>d_hash</code>中完成）。我注意到，对于每一个dentry，都有一个d_flags，它的可能的值，定义在dcache.h当中。因为dentry cache的大小也是有限的，因此我们也不可能无限制地把dentry保存在cache中。因此linux首先把dentry的状态进行了定义：  </p>
<p>free：该状态目录项对象不包含有效信息，未被VFS使用<br>unused：目前没有被内核使用，d_count的值为0，d_inode仍然指向相关的索引节点<br>in use：正在被使用，d_count的值大于0，d_inode仍然指向相关的索引节点<br>negative：与目录项关联的索引节点不存在，相应的磁盘索引节点已经被删除（d_inode为负数）。 </p>
<p>那么对于unused和negative这一类目录项，linux使用了一个LRU（最近最少使用）的双向链表，一旦dentry cache的大小吃紧，就从这个LRU链表中删除dentry。  </p>
<h4 id="索引节点对象"><a href="#索引节点对象" class="headerlink" title="索引节点对象"></a>索引节点对象</h4><p>索引节点对象，是VFS当中最为重要的一个数据结构。它的作用是<strong>表示文件的相关信息</strong>。这里的相关信息，不包括文件本身的内容，而是诸如文件大小、拥有者、创建时间等信息。在一个文件被首次访问时，内核会在内存中构造它的索引节点对象。我们在操作系统中，可以任意修改一个文件的名字，但是索引节点和文件是一一对应的（由索引节点号标识），只要文件存在它就会存在（注意，超级块对象和索引节点对象在硬盘上都是有对应的实体数据结构的，在使用时利用硬盘上的内容，在内存中构造索引节点对象）。目录项是用来找到一个对应的索引节点实体，而具体与文件关联的工作则是由索引节点完成的。<br>让我们来看看索引节点的数据结构（只取了重要的部分），其定义在fs.h当中：</p>
<pre><code>struct inode {
    struct hlist_node    i_hash;     /* 散列表，用于快速查找inode */
    struct list_head    i_list;        /* 相同状态索引节点链表 */
    struct list_head    i_sb_list;  /* 文件系统中所有节点链表  */
    struct list_head    i_dentry;   /* 目录项链表 */
    unsigned long        i_ino;      /* 节点号 */
    atomic_t        i_count;        /* 引用计数 */
    unsigned int        i_nlink;    /* 硬链接数 */
    uid_t            i_uid;          /* 使用者id */
    gid_t            i_gid;          /* 使用组id */
    struct timespec        i_atime;    /* 最后访问时间 */
    struct timespec        i_mtime;    /* 最后修改时间 */
    struct timespec        i_ctime;    /* 最后改变时间 */
    const struct inode_operations    *i_op;  /* 索引节点操作函数 */
    const struct file_operations    *i_fop;    /* 缺省的索引节点操作 */
    struct super_block    *i_sb;              /* 相关的超级块 */
    struct address_space    *i_mapping;     /* 相关的地址映射 */
    struct address_space    i_data;         /* 设备地址映射 */
    unsigned int        i_flags;            /* 文件系统标志 */
    void            *i_private;             /* fs 私有指针 */
    unsigned long i_state;
};
</code></pre><p>可以看到，inode同样采用了多个链表来保存。<code>i_hash</code>用来快速查找inode，<code>i_list</code>则是相同状态索引结点形成的双链表，这包含有未用索引节点链表，正在使用索引节点链表和脏索引节点链表等。<code>i_dentry</code>是所有使用该节点的dentry链表。值得注意的是，inode不仅仅包含了自身索引节点的操作函数<code>i_op</code>，还有指向（缺省）文件操作的指针<code>i_fop</code>。当然，inode还会和super_block有联系。<code>i_sb</code>是索引节点所在的超级块，而i_sb_list则是超级块中的所有节点的链表。<br>当在某个目录下创建、打开一个文件时，内核就会调用<code>create()</code>为这个文件创建一个inode。VFS通过inode的i_op-&gt;create()函数来完成这个工作；它将目录的inode、新打开文件的dentry、访问权限作为参数；<code>lookup()</code>函数用来查找指定文件的dentry，<code>link()</code>和<code>symlink()</code>分别用来创建硬链接和软链接。</p>
<h4 id="超级块对象"><a href="#超级块对象" class="headerlink" title="超级块对象"></a>超级块对象</h4><p>与前面几类对象不同的是，超级块对象表述的内容更加庞大一些：它表示的是一个“已安装的文件系统”。它在文件系统安装时建立，在文件系统卸载时删除。其定义在fs.h当中，这里我只列举出了较为关键的域。</p>
<pre><code>struct super_block {
        struct list_head        s_list;         //超级块链表的指针
        dev_t                   s_dev;          //设备标识符
        unsigned char           s_blocksize_bits;
        unsigned long           s_blocksize;
        loff_t                  s_maxbytes;     //文件的最长长度
        struct file_system_type *s_type;
        const struct super_operations   *s_op;  //超级块的操作
        const struct dquot_operations   *dq_op; 
        const struct quotactl_ops       *s_qcop;
        const struct export_operations *s_export_op;
        unsigned long           s_flags;    //安装标识
        struct dentry           *s_root;    //根目录的目录项
        int                     s_count;
        const struct xattr_handler **s_xattr;
        struct list_head        s_inodes;       //所有的inodes链（打开文件的inodes链）

        struct block_device     *s_bdev;    //块设备
          char s_id[32];                          //块设备名称
        u8 s_uuid[16];                          //UUID            fmode_t                 s_mode;

        char *s_subtype;
        const struct dentry_operations *s_d_op; //default d_op for dentries
        void *s_fs_indo;     //文件系统的信息指针
        };
</code></pre><p>在linux中，每个超级块代表一个已安装的文件系统。所有的超级块链表，是以一种双向环形链表的形式链接在一起的。其<code>prev</code>、<code>next</code>保存在<code>list_head</code>域中。超级块对象中，保存有其根目录的<code>dentry</code>，以及其所有的<code>inode</code>。<code>s_fs_info</code>则指向了文件系统的超级块信息。而对于超级块来说，同样定义有<code>s_op</code>，也即超级块的操作表。<br>超级块一般是储存在磁盘的特定扇区当中，但如果是基于内存对文件系统，比如proc、sysfs，则是保存在内存当中，而超级块对象，则是在使用时创建的，它保存在内存中。  </p>
<h4 id="硬链接和软链接与复制"><a href="#硬链接和软链接与复制" class="headerlink" title="硬链接和软链接与复制"></a>硬链接和软链接与复制</h4><p>linux里面，可以把文件分成三个部分：文件名（dentry），inode，数据。<br>复制的定义很明确，就是为这三个部分，都创建一个新的副本。<br>硬链接的本质是一个“文件名”，一个文件可能有多个“文件名”，inode并不包含文件名，而只是有一个索引节点号。硬链接实际上就是为链接文件创建一个新的dentry，并将dentry写入父目录的数据中，而硬链接所对应的inode依然没有变。所以删除硬链接只是删除了dentry，而inode结点数减少1而已。<br>软链接就是一个普通的文件，只不过它的数据保存的是另一个文件的路径。软链接的创建，调用了<code>__ext4_new_inode()</code>来创建一个新的inode，并把<code>dentry-&gt;name</code>作为了它的内容。也就是说，软链接也同时创建了这三个部分。<br>二者的区别在哪里呢？首先，硬链接共享了inode，因此它不能跨文件系统；但是软链接不受这个限制。由于相同的原因，硬链接只能对存在的文件进行创建，而软链接不是。而且硬链接有可能会在目录中引入循环，所以不能指向目录；但软链接不会，因为它有一个inode实体可以跟踪。不过不论删除软链接还是硬链接，都不会对原文件、具有相同inode号的文件造成影响。但如果原文件被删除，软链接会变成死链接，硬链接不会，因为inode的计数并没有变成0。<br><img src="https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%93%BE%E6%8E%A5.png?raw=true" alt="">    </p>
<h4 id="路径名查找"><a href="#路径名查找" class="headerlink" title="路径名查找"></a>路径名查找</h4><p>每当进程需要识别一个文件时，就把它的文件路径名，传递给某个VFS系统调用，比如<code>open()</code>。在路径查找中，有个辅助的数据结构：<code>nameidata</code>，它用来向函数传递参数，并且保存查找的结果：  </p>
<pre><code>struct nameidata {
    struct path    path;
    struct qstr    last;
    struct path    root;
    struct inode    *inode; /* path.dentry.d_inode */
    unsigned int    flags;
    unsigned    seq, m_seq;
    int        last_type;
    unsigned    depth;
    struct file    *base;
    char *saved_names[MAX_NESTED_LINKS + 1];
};
</code></pre><p>在查找完成后，<code>path</code>中保存了目录项，<code>depth</code>表示了当前路径的深度，<code>saved_names</code>保存了符号链接处理中的路径名。<br>路径查找的复杂性，主要体现在VFS系统的一些特点上：（1）必须对目录的访问权限进行检查；（2）文件名可能是符号链接；（3）要考虑符号链接可能带来的循环引用；（4）文件名可能是文件系统的安装点（5）路径名和进程的命名空间有关等。<br>路径名查找的入口是<code>path_lookup()</code>，它调用了<code>filename_lookup()</code>。这个函数对nameidata进行了简单的填充，随后调用<code>lookupat()</code>。<br><code>lookupat()</code>函数中通过一个循环，和<code>path_init()</code>函数，逐级向下进行查找，检查目录的访问权限，并且考虑符号链接等情况。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;虚拟文件系统（VFS）的思想和作用&quot;&gt;&lt;a href=&quot;#虚拟文件系统（VFS）的思想和作用&quot; class=&quot;headerlink&quot; title=&quot;虚拟文件系统（VFS）的思想和作用&quot;&gt;&lt;/a&gt;虚拟文件系统（VFS）的思想和作用&lt;/h4&gt;&lt;p&gt;实际上，看到&lt;str
    
    </summary>
    
    
      <category term="linux" scheme="http://sec-lbx.tk/tags/linux/"/>
    
      <category term="操作系统" scheme="http://sec-lbx.tk/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
